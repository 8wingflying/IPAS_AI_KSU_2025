## ViT(2021)
- ViT ==> Vision Transformer，它是一種直接將Transformer架構應用於圖像分類的模型。
- 基本概念：
  - ViT（Vision Transformer）是Google在2020年提出的一種模型，旨在將Transformer的自注意力機制應用到電腦視覺任務中，特別是圖像分類。
- 工作原理：
  - Patch Embedding：將輸入圖像分割成固定大小的patches（例如16x16圖元），每個patch通過線性投影（embedding層）轉換為token，形成序列輸入。
  - Position Embedding：為每個patch添加位置資訊，確保模型能理解patch的相對順序。
  - Self-Attention Mechanism：通過Transformer的encoder層處理這些patch token，捕捉長距離依賴關係。
  - Classification：通常在序列中添加一個特殊的[class token]，經過多層Transformer編碼後，通過MLP head進行分類。
- 優勢：
  - 能夠捕捉全域特徵，相比傳統CNN的局部感受野，ViT可以更自然地處理圖像。
  - 統一了NLP和CV的架構，為多模態任務提供了統一的處理框架。
- 挑戰：
  - 需要大量資料進行訓練，否則性能可能不如傳統的CNN模型。
  - 計算複雜度較高，尤其是在處理高解析度圖像時。
- 後續發展：
  - 改進版：如DeiT（Data-efficient Image Transformers）改進了訓練效率。
  - 混合架構：結合卷積（如Swin Transformer）以利用局部特徵。
  - 多模態應用：在多模態任務中表現出色，如圖文匹配、視頻理解等。
  - https://zhuanlan.zhihu.com/p/1906677913434161582
  - Swin Transformer
  - MViT (Multiscale Vision Transformers) 
- 應用場景：
  - 分類任務：在ImageNet等資料集上取得了與ResNet相媲美的甚至更好的性能。
  - 多模態任務：在結合文本資訊的多模態任務中表現優異，如CLIP模型。
- 局限性：
  - 在小資料集或需要精確圖元級細節的任務中，ViT的性能可能不如CNN。
  - 訓練和推理成本較高，對計算資源要求嚴格。
- 重要性
  - 通過ViT的引入，電腦視覺領域開始探索Transformer在視覺任務中的潛力，儘管面臨一些挑戰，但它為視覺任務提供了一種新的視角和方法。
## 🌟導讀
- https://hackmd.io/@YungHuiHsu/ByDHdxBS5
- https://developer.volcengine.com/articles/7382260930500886554
- https://blog.csdn.net/qq_39478403/article/details/118704747
- ViT (Vision Transformer) 概述與優勢: 對比CNN與Swin等hierarchical方法
- 深入解析Vision Transformer (ViT)並使用Python進行微調
- [論文導讀] Vision Transformer (ViT) 附程式碼實作
- https://blog.csdn.net/qq_42589613/article/details/145562944
## 範例
- 🤗[Vision Transformer (ViT)|Hugging Face](https://huggingface.co/docs/transformers/model_doc/vit)
```python
import torch
from transformers import pipeline

pipeline = pipeline(
    task="image-classification",
    model="google/vit-base-patch16-224",
    torch_dtype=torch.float16,
    device=0
)

pipeline(images="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg")
```
- 
