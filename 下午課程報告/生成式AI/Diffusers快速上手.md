## ğŸ¤— Diffusers
- https://github.com/huggingface/diffusers
- ğŸ¤— Diffusers æ˜¯æœ€å…ˆé€²çš„é è¨“ç·´æ“´æ•£æ¨¡å‹çš„é¦–é¸åº«ï¼Œç”¨æ–¼ç”Ÿæˆåœ–åƒã€éŸ³è¨Šç”šè‡³åˆ†å­çš„ 3D çµæ§‹ã€‚
- ç„¡è«–æ‚¨æ˜¯åœ¨å°‹æ‰¾ç°¡å–®çš„æ¨ç†è§£æ±ºæ–¹æ¡ˆé‚„æ˜¯è¨“ç·´è‡ªå·±çš„æ“´æ•£æ¨¡å‹ï¼Œ ğŸ¤— Diffusers éƒ½æ˜¯ä¸€å€‹æ”¯æ´å…©è€…çš„æ¨¡çµ„åŒ–å·¥å…·ç®±ã€‚
- æˆ‘å€‘çš„åº«åœ¨è¨­è¨ˆæ™‚æ³¨é‡å¯ç”¨æ€§è€Œä¸æ˜¯æ€§èƒ½ï¼Œç°¡å–®è€Œä¸æ˜¯ç°¡å–®ï¼Œä»¥åŠå¯å®šè£½æ€§è€Œä¸æ˜¯æŠ½è±¡ã€‚

## ğŸ¤— Diffusers æä¾›ä¸‰å€‹æ ¸å¿ƒå…ƒä»¶ï¼š
- `æœ€å…ˆé€²çš„æ“´æ•£ç®¡é“(State-of-the-art diffusion pipelines)` ==> åªéœ€å¹¾è¡Œä»£ç¢¼å³å¯åœ¨æ¨ç†ä¸­é‹è¡Œã€‚
  - The DiffusionPipeline is a high-level end-to-end class designed to rapidly generate samples from pretrained diffusion models for inference.
- `å¯äº’æ›çš„é›œè¨Šèª¿åº¦å™¨(Interchangeable noise schedulers)` ==> é©ç”¨æ–¼ä¸åŒçš„æ“´æ•£é€Ÿåº¦å’Œè¼¸å‡ºå“è³ªã€‚
  - Many different schedulers - algorithms that control how noise is added for training, and how to generate denoised images during inference.
- `é è¨“ç·´æ¨¡å‹(Pretrained models)`å¯ç”¨ä½œæ§‹å»ºå€å¡Š(building blocks)ï¼Œä¸¦èˆ‡`èª¿åº¦å™¨(schedulers)`çµåˆä½¿ç”¨ï¼Œç”¨æ–¼å‰µå»ºæ‚¨è‡ªå·±çš„ç«¯åˆ°ç«¯æ“´æ•£ç³»çµ±ã€‚
  - Popular pretrained model architectures and modules that can be used as building blocks for creating diffusion systems.

## diffusers_å¯¦æˆ°1
- https://github.com/huggingface/diffusers
- [ä½¿ç”¨T4 GPU](diffusers_å¯¦æˆ°1_20250609_2.ipynb)
```python
from diffusers import DiffusionPipeline
import torch

pipeline = DiffusionPipeline.from_pretrained("stable-diffusion-v1-5/stable-diffusion-v1-5", torch_dtype=torch.float16)
pipeline.to("cuda")
pipeline("An image of a squirrel in Picasso style").images[0]
```

```python
from diffusers import DDPMScheduler, UNet2DModel
from PIL import Image
import torch

scheduler = DDPMScheduler.from_pretrained("google/ddpm-cat-256")
model = UNet2DModel.from_pretrained("google/ddpm-cat-256").to("cuda")
scheduler.set_timesteps(50)

sample_size = model.config.sample_size
noise = torch.randn((1, 3, sample_size, sample_size), device="cuda")
input = noise

for t in scheduler.timesteps:
    with torch.no_grad():
        noisy_residual = model(input, t).sample
        prev_noisy_sample = scheduler.step(noisy_residual, t, input).prev_sample
        input = prev_noisy_sample

image = (input / 2 + 0.5).clamp(0, 1)
image = image.cpu().permute(0, 2, 3, 1).numpy()[0]
image = Image.fromarray((image * 255).round().astype("uint8"))
image
```
