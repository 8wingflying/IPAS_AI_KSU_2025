## DDPM 
- [DDPMs from scratch](https://magic-with-latents.github.io/latent/ddpms-series.html)
  - ğŸ‘å¾ˆå¥½çš„é«˜æ–¯åˆ†å¸ƒæ•™å­¸ [Gaussian Distribution and DDPMs](https://github.com/AakashKumarNain/diffusion_models/blob/main/notebooks/all_you_need_to_know_about_gaussian.ipynb)
  -  [A deep dive into DDPMs](https://magic-with-latents.github.io/latent/posts/ddpms/part3/

#### ã€Kerasç¯„ä¾‹ç¨‹å¼ã€‘[Denoising Diffusion Probabilistic Model](https://keras.io/examples/generative/ddpm/)
- ç”Ÿæˆå»ºæ¨¡åœ¨éå»äº”å¹´ä¸­ç¶“æ­·äº†å·¨å¤§çš„å¢é•·ã€‚
- äº‹å¯¦è­‰æ˜ï¼ŒVAEã€GAN å’ŒåŸºæ–¼æµçš„æ¨¡å‹åœ¨ç”Ÿæˆé«˜è³ªé‡çš„åœ–åƒå…§å®¹ã€‚
- æ“´æ•£æ¨¡å‹æ˜¯ä¸€ç¨®æ–°å‹çš„ç”Ÿæˆæ¨¡å‹æ¨¡å‹ã€‚
- æ“´æ•£æ¨¡å‹å—åˆ°éå¹³è¡¡ç†±åŠ›å­¸çš„å•Ÿç™¼ï¼Œå®ƒå€‘å­¸ç¿’é€šéé™å™ªç”Ÿæˆã€‚
- é™å™ªå­¸ç¿’åŒ…æ‹¬å…©å€‹éç¨‹ï¼Œ æ¯å€‹éƒ½æ˜¯ç‘ªå¾‹å¯å¤«éˆ:
  - æ­£å‘éç¨‹ï¼šåœ¨æ­£å‘éç¨‹ä¸­ï¼Œæˆ‘å€‘åœ¨ä¸€ç³»åˆ—æ™‚é–“æ­¥é©Ÿæ…¢æ…¢åœ°å‘æ•¸æ“šä¸­æ·»åŠ éš¨æ©Ÿé›œè¨Š ã€‚
    - ç•¶å‰æ™‚é–“æ­¥é•·çš„æ¨£æœ¬æ˜¯å¾é«˜æ–¯åˆ†ä½ˆä¸­æå–ï¼Œå…¶ä¸­åˆ†ä½ˆçš„å‡å€¼æ˜¯æœ‰æ¢ä»¶çš„
    - åœ¨ä¸Šä¸€å€‹æ™‚é–“æ­¥é•·çš„æ¨£æœ¬ä¸Šï¼Œåˆ†ä½ˆçš„æ–¹å·®å¦‚ä¸‹ å›ºå®šçš„æ™‚ç¨‹è¡¨ã€‚
    - åœ¨æ­£å‘éç¨‹çµæŸæ™‚ï¼Œæ¨£æœ¬æœ€çµ‚æœƒå¾—åˆ°ä¸€å€‹ç´”é›œè¨Šåˆ†ä½ˆã€‚(t1, t2, ..., tn )
  - åå‘éç¨‹ï¼šåœ¨åå‘éç¨‹ä¸­ï¼Œæˆ‘å€‘å˜—è©¦æ’¤æ¶ˆåœ¨ æ¯å€‹æ™‚é–“æ­¥ã€‚
    - æˆ‘å€‘å¾ç´”é›œè¨Šåˆ†ä½ˆé–‹å§‹ï¼ˆ forward processï¼‰ ä¸¦å˜—è©¦å‘å¾Œæ–¹å‘å°æ¨£æœ¬é€²è¡Œå»å™ªã€‚(tn, tn-1, ..., t1)

- é€™å€‹ç¯„ä¾‹ç¨‹å¼å¯¦ä½œ Denoising Diffusion Probabilistic Models(DDPM)ã€‚
- é€™æ˜¯ç¬¬ä¸€ç¯‡ä½¿ç”¨æ“´æ•£æ¨¡å‹ç”Ÿæˆé«˜å“è³ªåœ–åƒçš„è«–æ–‡ ã€‚
- ä½œè€…è­‰æ˜æ“´æ•£æ¨¡å‹çš„æŸå€‹åƒæ•¸åŒ–æ­ç¤ºäº†èˆ‡ åœ¨è¨“ç·´æœŸé–“å’Œé€€ç«æœŸé–“åœ¨å¤šå€‹é›œè¨Šç´šåˆ¥ä¸Šé€²è¡Œé™å™ªåˆ†æ•¸åŒ¹é… æ¡æ¨£æœŸé–“çš„ Langevin å‹•æ…‹ï¼Œå¯ç”Ÿæˆæœ€ä½³è³ªé‡çµæœã€‚
- æœ¬æ–‡è¤‡è£½äº†ç‘ªå¾‹å¯å¤«éˆï¼ˆæ­£å‘éç¨‹å’Œåå‘éç¨‹ï¼‰ åƒèˆ‡æ“´æ•£éç¨‹ï¼Œä½†ç”¨æ–¼åœ–åƒã€‚forward é€²ç¨‹æ˜¯å›ºå®šçš„ï¼Œ
- æœ¬æ–‡ä»‹ç´¹äº†å…©ç¨®æ¼”ç®—æ³•ï¼Œä¸€ç¨®ç”¨æ–¼è¨“ç·´æ¨¡å‹ï¼Œå¦ä¸€ç¨®ç”¨æ–¼ å¾ç¶“éè¨“ç·´çš„æ¨¡å‹ä¸­æ¡æ¨£ã€‚
- é€šéå„ªåŒ–é€šå¸¸çš„ è² å°æ•¸ä¼¼ç„¶çš„è®Šåˆ†ç•Œã€‚
- ç›®æ¨™å‡½æ•¸æ›´é€²ä¸€æ­¥ ç°¡åŒ–ï¼Œä¸¦å°‡ç¶²è·¯è¦–ç‚ºé›œè¨Šé æ¸¬ç¶²è·¯ã€‚
- å„ªåŒ–åï¼Œ æˆ‘å€‘å¯ä»¥å¾ç¶²è·¯ä¸­æ¡æ¨£ï¼Œå¾é›œè¨Šæ¨£æœ¬ä¸­ç”Ÿæˆæ–°åœ–åƒã€‚
- é€™æ˜¯ä¸€å€‹ è«–æ–‡ä¸­ä»‹ç´¹çš„å…©ç¨®æ¼”ç®—æ³•çš„æ¦‚è¿°ï¼š



- æ³¨æ„äº‹é …ï¼š
- DDPM åªæ˜¯å¯¦ç¾æ“´æ•£æ¨¡å‹çš„ä¸€ç¨®æ–¹å¼ã€‚
- æ­¤å¤–ï¼Œæ¡æ¨£ æ¼”ç®—æ³•è¤‡è£½å®Œæ•´çš„ç‘ªå¾‹å¯å¤«éˆã€‚å› æ­¤ï¼Œå®ƒçš„é€Ÿåº¦å¾ˆæ…¢ ç”Ÿæˆæ–°æ¨£æœ¬ï¼Œèˆ‡ GAN ç­‰å…¶ä»–ç”Ÿæˆæ¨¡å‹é€²è¡Œæ¯”è¼ƒã€‚
- å¤§é‡ç ”ç©¶ å·²åšå‡ºåŠªåŠ›ä¾†è§£æ±ºæ­¤å•é¡Œã€‚
- ä¸€å€‹é€™æ¨£çš„ä¾‹å­æ˜¯å»å™ªæ“´æ•£ éš±å¼æ¨¡å‹ï¼Œæˆ–ç°¡ç¨± DDIMï¼Œå…¶ä¸­ä½œè€…å°‡ç‘ªå¾‹å¯å¤«éˆæ›¿æ›ç‚º éç‘ªå¾‹å¯å¤«éç¨‹ä¾†æ›´å¿«åœ°æ¡æ¨£ã€‚

- DDPM æ¨¡å‹çš„å¯¦ä½œ: ==>å¾ˆç°¡å–®
  - æˆ‘å€‘å®šç¾©äº†ä¸€å€‹æ¨¡å‹ï¼Œè©²æ¨¡å‹æ¡ç”¨ å…©å€‹è¼¸å…¥ï¼šImages å’Œéš¨æ©Ÿæ¡æ¨£çš„æ™‚é–“æ­¥é•·ã€‚
  - åœ¨æ¯å€‹è¨“ç·´æ­¥é©Ÿä¸­ï¼Œæˆ‘å€‘ åŸ·è¡Œä»¥ä¸‹ä½œä¾†è¨“ç·´æˆ‘å€‘çš„æ¨¡å‹ï¼š
    - è¦æ·»åŠ åˆ° inputs çš„éš¨æ©Ÿé›œè¨Šæ¡æ¨£ã€‚
    - æ‡‰ç”¨æ­£å‘è™•ç†ä»¥æ“´æ•£å…·æœ‰æ¡æ¨£é›œè¨Šçš„è¼¸å…¥ã€‚
    - æ‚¨çš„æ¨¡å‹å°‡é€™äº›é›œè¨Šæ¨£æœ¬ä½œç‚ºè¼¸å…¥ä¸¦è¼¸å‡ºé›œè¨Š æ¯å€‹æ™‚é–“æ­¥çš„ predictionã€‚
    - çµ¦å®šçœŸå¯¦é›œè¨Šå’Œé æ¸¬é›œè¨Šï¼Œæˆ‘å€‘è¨ˆç®—æå¤±å€¼
    - ç„¶å¾Œï¼Œæˆ‘å€‘è¨ˆç®—æ¢¯åº¦ä¸¦æ›´æ–°æ¨¡å‹æ¬Šé‡ã€‚
  
  - é‘’æ–¼æˆ‘å€‘çš„æ¨¡å‹çŸ¥é“å¦‚ä½•åœ¨çµ¦å®šçš„æ™‚é–“æ­¥é•·å°é›œè¨Šæ¨£æœ¬é€²è¡Œé™å™ªï¼Œ æˆ‘å€‘å¯ä»¥åˆ©ç”¨é€™å€‹æƒ³æ³•å¾ç´”é›œè¨Šé–‹å§‹ç”Ÿæˆæ–°æ¨£æœ¬ åˆ†é…ã€‚

## ç¯„ä¾‹ç¨‹å¼è§£èªª
```python
import math
import numpy as np
import matplotlib.pyplot as plt

# Requires TensorFlow >=2.11 for the GroupNormalization layer.
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import tensorflow_datasets as tfds
```

```python
"""
## åƒæ•¸è¨­å®š Hyperparameters
"""

batch_size = 32
num_epochs = 1  # Just for the sake of demonstration
total_timesteps = 1000
norm_groups = 8  # Number of groups used in GroupNormalization layer
learning_rate = 2e-4

img_size = 64
img_channels = 3
clip_min = -1.0
clip_max = 1.0

first_conv_channels = 64
channel_multiplier = [1, 2, 4, 8]
widths = [first_conv_channels * mult for mult in channel_multiplier]
has_attention = [False, False, True, True]
num_res_blocks = 2  # Number of residual blocks

## ä½¿ç”¨è³‡æ–™é›† oxford_flowers102
dataset_name = "oxford_flowers102"
splits = ["train"]

```


```python
## ä½¿ç”¨è³‡æ–™é›† ==> è³‡æ–™é è™•ç†
# Load the dataset
(ds,) = tfds.load(dataset_name, split=splits, with_info=False, shuffle_files=True)


def augment(img):
    """Flips an image left/right randomly."""
    return tf.image.random_flip_left_right(img)


def resize_and_rescale(img, size):
    """Resize the image to the desired size first and then
    rescale the pixel values in the range [-1.0, 1.0].

    Args:
        img: Image tensor
        size: Desired image size for resizing
    Returns:
        Resized and rescaled image tensor
    """

    height = tf.shape(img)[0]
    width = tf.shape(img)[1]
    crop_size = tf.minimum(height, width)

    img = tf.image.crop_to_bounding_box(
        img,
        (height - crop_size) // 2,
        (width - crop_size) // 2,
        crop_size,
        crop_size,
    )

    # Resize
    img = tf.cast(img, dtype=tf.float32)
    img = tf.image.resize(img, size=size, antialias=True)

    # Rescale the pixel values
    img = img / 127.5 - 1.0
    img = tf.clip_by_value(img, clip_min, clip_max)
    return img


def train_preprocessing(x):
    img = x["image"]
    img = resize_and_rescale(img, size=(img_size, img_size))
    img = augment(img)
    return img


train_ds = (
    ds.map(train_preprocessing, num_parallel_calls=tf.data.AUTOTUNE)
    .batch(batch_size, drop_remainder=True)
    .shuffle(batch_size * 2)
    .prefetch(tf.data.AUTOTUNE)
)
```
#### Gaussian diffusion utilities
- å®šç¾©æ­£å‘éç¨‹å’Œåå‘éç¨‹
- ä½œç‚ºå–®ç¨çš„å¯¦ç”¨ç¨‹å¼ã€‚
- æ­¤ç¨‹å¼ä¸­çš„å¤§éƒ¨åˆ†ä»£ç¢¼æ˜¯å¾åŸå§‹å¯¦ç¾ç‚ºåŸºç¤åƒ…ç¨ä½œä¿®æ”¹ã€‚
```python
class GaussianDiffusion:
    """Gaussian diffusion utility.

    Args:
        beta_start: Start value of the scheduled variance
        beta_end: End value of the scheduled variance
        timesteps: Number of time steps in the forward process
    """

    def __init__(
        self,
        beta_start=1e-4,
        beta_end=0.02,
        timesteps=1000,
        clip_min=-1.0,
        clip_max=1.0,
    ):
        self.beta_start = beta_start
        self.beta_end = beta_end
        self.timesteps = timesteps
        self.clip_min = clip_min
        self.clip_max = clip_max

        # Define the linear variance schedule
        self.betas = betas = np.linspace(
            beta_start,
            beta_end,
            timesteps,
            dtype=np.float64,  # Using float64 for better precision
        )
        self.num_timesteps = int(timesteps)

        alphas = 1.0 - betas
        alphas_cumprod = np.cumprod(alphas, axis=0)
        alphas_cumprod_prev = np.append(1.0, alphas_cumprod[:-1])

        self.betas = tf.constant(betas, dtype=tf.float32)
        self.alphas_cumprod = tf.constant(alphas_cumprod, dtype=tf.float32)
        self.alphas_cumprod_prev = tf.constant(alphas_cumprod_prev, dtype=tf.float32)

        # Calculations for diffusion q(x_t | x_{t-1}) and others
        self.sqrt_alphas_cumprod = tf.constant(
            np.sqrt(alphas_cumprod), dtype=tf.float32
        )

        self.sqrt_one_minus_alphas_cumprod = tf.constant(
            np.sqrt(1.0 - alphas_cumprod), dtype=tf.float32
        )

        self.log_one_minus_alphas_cumprod = tf.constant(
            np.log(1.0 - alphas_cumprod), dtype=tf.float32
        )

        self.sqrt_recip_alphas_cumprod = tf.constant(
            np.sqrt(1.0 / alphas_cumprod), dtype=tf.float32
        )
        self.sqrt_recipm1_alphas_cumprod = tf.constant(
            np.sqrt(1.0 / alphas_cumprod - 1), dtype=tf.float32
        )

        # Calculations for posterior q(x_{t-1} | x_t, x_0)
        posterior_variance = (
            betas * (1.0 - alphas_cumprod_prev) / (1.0 - alphas_cumprod)
        )
        self.posterior_variance = tf.constant(posterior_variance, dtype=tf.float32)

        # Log calculation clipped because the posterior variance is 0 at the beginning
        # of the diffusion chain
        self.posterior_log_variance_clipped = tf.constant(
            np.log(np.maximum(posterior_variance, 1e-20)), dtype=tf.float32
        )

        self.posterior_mean_coef1 = tf.constant(
            betas * np.sqrt(alphas_cumprod_prev) / (1.0 - alphas_cumprod),
            dtype=tf.float32,
        )

        self.posterior_mean_coef2 = tf.constant(
            (1.0 - alphas_cumprod_prev) * np.sqrt(alphas) / (1.0 - alphas_cumprod),
            dtype=tf.float32,
        )

    def _extract(self, a, t, x_shape):
        """Extract some coefficients at specified timesteps,
        then reshape to [batch_size, 1, 1, 1, 1, ...] for broadcasting purposes.

        Args:
            a: Tensor to extract from
            t: Timestep for which the coefficients are to be extracted
            x_shape: Shape of the current batched samples
        """
        batch_size = x_shape[0]
        out = tf.gather(a, t)
        return tf.reshape(out, [batch_size, 1, 1, 1])

    def q_mean_variance(self, x_start, t):
        """Extracts the mean, and the variance at current timestep.

        Args:
            x_start: Initial sample (before the first diffusion step)
            t: Current timestep
        """
        x_start_shape = tf.shape(x_start)
        mean = self._extract(self.sqrt_alphas_cumprod, t, x_start_shape) * x_start
        variance = self._extract(1.0 - self.alphas_cumprod, t, x_start_shape)
        log_variance = self._extract(
            self.log_one_minus_alphas_cumprod, t, x_start_shape
        )
        return mean, variance, log_variance

    def q_sample(self, x_start, t, noise):
        """Diffuse the data.

        Args:
            x_start: Initial sample (before the first diffusion step)
            t: Current timestep
            noise: Gaussian noise to be added at the current timestep
        Returns:
            Diffused samples at timestep `t`
        """
        x_start_shape = tf.shape(x_start)
        return (
            self._extract(self.sqrt_alphas_cumprod, t, tf.shape(x_start)) * x_start
            + self._extract(self.sqrt_one_minus_alphas_cumprod, t, x_start_shape)
            * noise
        )

    def predict_start_from_noise(self, x_t, t, noise):
        x_t_shape = tf.shape(x_t)
        return (
            self._extract(self.sqrt_recip_alphas_cumprod, t, x_t_shape) * x_t
            - self._extract(self.sqrt_recipm1_alphas_cumprod, t, x_t_shape) * noise
        )

    def q_posterior(self, x_start, x_t, t):
        """Compute the mean and variance of the diffusion
        posterior q(x_{t-1} | x_t, x_0).

        Args:
            x_start: Stating point(sample) for the posterior computation
            x_t: Sample at timestep `t`
            t: Current timestep
        Returns:
            Posterior mean and variance at current timestep
        """

        x_t_shape = tf.shape(x_t)
        posterior_mean = (
            self._extract(self.posterior_mean_coef1, t, x_t_shape) * x_start
            + self._extract(self.posterior_mean_coef2, t, x_t_shape) * x_t
        )
        posterior_variance = self._extract(self.posterior_variance, t, x_t_shape)
        posterior_log_variance_clipped = self._extract(
            self.posterior_log_variance_clipped, t, x_t_shape
        )
        return posterior_mean, posterior_variance, posterior_log_variance_clipped

    def p_mean_variance(self, pred_noise, x, t, clip_denoised=True):
        x_recon = self.predict_start_from_noise(x, t=t, noise=pred_noise)
        if clip_denoised:
            x_recon = tf.clip_by_value(x_recon, self.clip_min, self.clip_max)

        model_mean, posterior_variance, posterior_log_variance = self.q_posterior(
            x_start=x_recon, x_t=x, t=t
        )
        return model_mean, posterior_variance, posterior_log_variance

    def p_sample(self, pred_noise, x, t, clip_denoised=True):
        """Sample from the diffusion model.

        Args:
            pred_noise: Noise predicted by the diffusion model
            x: Samples at a given timestep for which the noise was predicted
            t: Current timestep
            clip_denoised (bool): Whether to clip the predicted noise
                within the specified range or not.
        """
        model_mean, _, model_log_variance = self.p_mean_variance(
            pred_noise, x=x, t=t, clip_denoised=clip_denoised
        )
        noise = tf.random.normal(shape=x.shape, dtype=x.dtype)
        # No noise when t == 0
        nonzero_mask = tf.reshape(
            1 - tf.cast(tf.equal(t, 0), tf.float32), [tf.shape(x)[0], 1, 1, 1]
        )
        return model_mean + nonzero_mask * tf.exp(0.5 * model_log_variance) * noise
```
#### ç¶²è·¯æ¶æ§‹(Network architecture)è¨­å®š==>ä»¥U-Netç‚ºåŸºç¤ç¨ä½œå°æ”¹
- U-Net æœ€åˆæ˜¯ç‚ºèªç¾©åˆ†å‰²è€Œé–‹ç™¼çš„ï¼Œæ˜¯ä¸€ç¨®å»£æ³›ç”¨æ–¼å¯¦ç¾æ“´æ•£æ¨¡å‹
- æœ¬ç¯„ä¾‹æœ‰ä¸€äº›ç´°å¾®çš„ä¿®æ”¹ï¼š
  - è©²ç¶²è·¯æ¥å—å…©å€‹è¼¸å…¥ï¼šåœ–åƒå’Œæ™‚é–“æ­¥é•·
  - ä¸€æ—¦æˆ‘å€‘é”åˆ°ç‰¹å®šè§£æåº¦ï¼Œæ²ç©å¡Šä¹‹é–“çš„è‡ªæˆ‘æ³¨æ„ ï¼ˆè«–æ–‡ä¸­ç‚º 16x16ï¼‰
  - çµ„æ­¸ä¸€åŒ–è€Œä¸æ˜¯æ¬Šé‡æ­¸ä¸€åŒ–
- æœ¬ç¯„ä¾‹å¯¦ä½œåŸå§‹è«–æ–‡ä¸­ä½¿ç”¨çš„å¤§éƒ¨åˆ†å…§å®¹ã€‚
- åœ¨æ•´å€‹ç¶²è·¯ä¸­ä½¿ç”¨å•Ÿå‹•å‡½æ•¸ã€‚
- æˆ‘å€‘ä½¿ç”¨æ–¹å·®å°ºåº¦ kernel initializer ä¸­ã€‚swish
- æ­¤è™•çš„å”¯ä¸€å€åˆ¥æ˜¯ç”¨æ–¼åœ–å±¤çš„çµ„æ•¸ã€‚å°æ–¼ flowers æ•¸æ“šé›†ï¼Œ æˆ‘å€‘ç™¼ç¾ å€¼ of æœƒç”¢ç”Ÿæ›´å¥½çš„çµæœ èˆ‡é è¨­å€¼ ç›¸æ¯”ã€‚
- dropout æ˜¯å¯é¸çš„ï¼Œæ‡‰è©²æ˜¯ åœ¨éåº¦æ“¬åˆçš„å¯èƒ½æ€§å¾ˆé«˜æ™‚ä½¿ç”¨ã€‚
- åœ¨è«–æ–‡ä¸­ï¼Œä½œè€…ä½¿ç”¨äº†dropout åƒ…åœ¨ CIFAR10 ä¸Šè¨“ç·´æ™‚ã€‚GroupNormalizationgroups=8groups=32
```python
# Kernel initializer to use
def kernel_init(scale):
    scale = max(scale, 1e-10)
    return keras.initializers.VarianceScaling(
        scale, mode="fan_avg", distribution="uniform"
    )


class AttentionBlock(layers.Layer):
    """Applies self-attention.

    Args:
        units: Number of units in the dense layers
        groups: Number of groups to be used for GroupNormalization layer
    """

    def __init__(self, units, groups=8, **kwargs):
        self.units = units
        self.groups = groups
        super().__init__(**kwargs)

        self.norm = layers.GroupNormalization(groups=groups)
        self.query = layers.Dense(units, kernel_initializer=kernel_init(1.0))
        self.key = layers.Dense(units, kernel_initializer=kernel_init(1.0))
        self.value = layers.Dense(units, kernel_initializer=kernel_init(1.0))
        self.proj = layers.Dense(units, kernel_initializer=kernel_init(0.0))

    def call(self, inputs):
        batch_size = tf.shape(inputs)[0]
        height = tf.shape(inputs)[1]
        width = tf.shape(inputs)[2]
        scale = tf.cast(self.units, tf.float32) ** (-0.5)

        inputs = self.norm(inputs)
        q = self.query(inputs)
        k = self.key(inputs)
        v = self.value(inputs)

        attn_score = tf.einsum("bhwc, bHWc->bhwHW", q, k) * scale
        attn_score = tf.reshape(attn_score, [batch_size, height, width, height * width])

        attn_score = tf.nn.softmax(attn_score, -1)
        attn_score = tf.reshape(attn_score, [batch_size, height, width, height, width])

        proj = tf.einsum("bhwHW,bHWc->bhwc", attn_score, v)
        proj = self.proj(proj)
        return inputs + proj


class TimeEmbedding(layers.Layer):
    def __init__(self, dim, **kwargs):
        super().__init__(**kwargs)
        self.dim = dim
        self.half_dim = dim // 2
        self.emb = math.log(10000) / (self.half_dim - 1)
        self.emb = tf.exp(tf.range(self.half_dim, dtype=tf.float32) * -self.emb)

    def call(self, inputs):
        inputs = tf.cast(inputs, dtype=tf.float32)
        emb = inputs[:, None] * self.emb[None, :]
        emb = tf.concat([tf.sin(emb), tf.cos(emb)], axis=-1)
        return emb


def ResidualBlock(width, groups=8, activation_fn=keras.activations.swish):
    def apply(inputs):
        x, t = inputs
        input_width = x.shape[3]

        if input_width == width:
            residual = x
        else:
            residual = layers.Conv2D(
                width, kernel_size=1, kernel_initializer=kernel_init(1.0)
            )(x)

        temb = activation_fn(t)
        temb = layers.Dense(width, kernel_initializer=kernel_init(1.0))(temb)[
            :, None, None, :
        ]

        x = layers.GroupNormalization(groups=groups)(x)
        x = activation_fn(x)
        x = layers.Conv2D(
            width, kernel_size=3, padding="same", kernel_initializer=kernel_init(1.0)
        )(x)

        x = layers.Add()([x, temb])
        x = layers.GroupNormalization(groups=groups)(x)
        x = activation_fn(x)

        x = layers.Conv2D(
            width, kernel_size=3, padding="same", kernel_initializer=kernel_init(0.0)
        )(x)
        x = layers.Add()([x, residual])
        return x

    return apply


def DownSample(width):
    def apply(x):
        x = layers.Conv2D(
            width,
            kernel_size=3,
            strides=2,
            padding="same",
            kernel_initializer=kernel_init(1.0),
        )(x)
        return x

    return apply


def UpSample(width, interpolation="nearest"):
    def apply(x):
        x = layers.UpSampling2D(size=2, interpolation=interpolation)(x)
        x = layers.Conv2D(
            width, kernel_size=3, padding="same", kernel_initializer=kernel_init(1.0)
        )(x)
        return x

    return apply


def TimeMLP(units, activation_fn=keras.activations.swish):
    def apply(inputs):
        temb = layers.Dense(
            units, activation=activation_fn, kernel_initializer=kernel_init(1.0)
        )(inputs)
        temb = layers.Dense(units, kernel_initializer=kernel_init(1.0))(temb)
        return temb

    return apply


def build_model(
    img_size,
    img_channels,
    widths,
    has_attention,
    num_res_blocks=2,
    norm_groups=8,
    interpolation="nearest",
    activation_fn=keras.activations.swish,
):
    image_input = layers.Input(
        shape=(img_size, img_size, img_channels), name="image_input"
    )
    time_input = keras.Input(shape=(), dtype=tf.int64, name="time_input")

    x = layers.Conv2D(
        first_conv_channels,
        kernel_size=(3, 3),
        padding="same",
        kernel_initializer=kernel_init(1.0),
    )(image_input)

    temb = TimeEmbedding(dim=first_conv_channels * 4)(time_input)
    temb = TimeMLP(units=first_conv_channels * 4, activation_fn=activation_fn)(temb)

    skips = [x]

    # DownBlock
    for i in range(len(widths)):
        for _ in range(num_res_blocks):
            x = ResidualBlock(
                widths[i], groups=norm_groups, activation_fn=activation_fn
            )([x, temb])
            if has_attention[i]:
                x = AttentionBlock(widths[i], groups=norm_groups)(x)
            skips.append(x)

        if widths[i] != widths[-1]:
            x = DownSample(widths[i])(x)
            skips.append(x)

    # MiddleBlock
    x = ResidualBlock(widths[-1], groups=norm_groups, activation_fn=activation_fn)(
        [x, temb]
    )
    x = AttentionBlock(widths[-1], groups=norm_groups)(x)
    x = ResidualBlock(widths[-1], groups=norm_groups, activation_fn=activation_fn)(
        [x, temb]
    )

    # UpBlock
    for i in reversed(range(len(widths))):
        for _ in range(num_res_blocks + 1):
            x = layers.Concatenate(axis=-1)([x, skips.pop()])
            x = ResidualBlock(
                widths[i], groups=norm_groups, activation_fn=activation_fn
            )([x, temb])
            if has_attention[i]:
                x = AttentionBlock(widths[i], groups=norm_groups)(x)

        if i != 0:
            x = UpSample(widths[i], interpolation=interpolation)(x)

    # End block
    x = layers.GroupNormalization(groups=norm_groups)(x)
    x = activation_fn(x)
    x = layers.Conv2D(3, (3, 3), padding="same", kernel_initializer=kernel_init(0.0))(x)
    return keras.Model([image_input, time_input], x, name="unet")
```
#### è¨“ç·´
- æˆ‘å€‘éµå¾ªç›¸åŒçš„è¨­ç½®ä¾†è¨“ç·´ diffusion æ¨¡å‹ï¼Œå¦‚å‰æ‰€è¿° åœ¨å ±ç´™ä¸Šã€‚
- æˆ‘å€‘ä½¿ç”¨å­¸ç¿’ç‡ç‚ºçš„ optimizerã€‚ æˆ‘å€‘å°è¡°æ¸›å› æ•¸ç‚º 0.999 çš„æ¨¡å‹åƒæ•¸ä½¿ç”¨ EMAã€‚
- æˆ‘å€‘ å°‡æˆ‘å€‘çš„æ¨¡å‹è¦–ç‚ºé›œè¨Šé æ¸¬ç¶²è·¯ï¼Œå³åœ¨æ¯å€‹è¨“ç·´æ­¥é©Ÿä¸­ï¼Œæˆ‘å€‘ å°‡ä¸€æ‰¹åœ–åƒå’Œç›¸æ‡‰çš„æ™‚é–“æ­¥é•·è¼¸å…¥åˆ°æˆ‘å€‘çš„UNetä¸­ï¼Œ ç¶²è·¯å°‡é›œè¨Šè¼¸å‡ºç‚ºé æ¸¬ã€‚Adam2e-4
- å”¯ä¸€çš„å€åˆ¥æ˜¯æˆ‘å€‘æ²’æœ‰ä½¿ç”¨ Kernel Inception Distance ï¼ˆKIDï¼‰ æˆ– Frechet èµ·å§‹è·é›¢ ï¼ˆFIDï¼‰ ç”¨æ–¼è©•ä¼°ç”Ÿæˆçš„ æ¨£æœ¬ã€‚
- é€™æ˜¯å› ç‚ºé€™å…©å€‹æŒ‡æ¨™éƒ½æ˜¯è¨ˆç®—å¯†é›†å‹çš„ ï¼Œç‚ºäº†ç°¡åŒ–å¯¦ç¾ï¼Œå°‡è·³éã€‚

**æ³¨æ„ï¼š** æˆ‘å€‘ä½¿ç”¨å‡æ–¹èª¤å·®ä½œç‚ºæå¤±å‡½æ•¸ï¼Œå®ƒèˆ‡ è«–æ–‡ï¼Œç†è«–ä¸Šæ˜¯æœ‰é“ç†çš„ã€‚ä¸éï¼Œåœ¨å¯¦è¸ä¸­ï¼Œé€™ç¨®æƒ…æ³ä¹Ÿå¾ˆå¸¸è¦‹ ä½¿ç”¨å¹³å‡çµ•å°èª¤å·®æˆ– Huber æå¤±ä½œç‚ºæå¤±å‡½æ•¸ã€‚

```python
class DiffusionModel(keras.Model):
    def __init__(self, network, ema_network, timesteps, gdf_util, ema=0.999):
        super().__init__()
        self.network = network
        self.ema_network = ema_network
        self.timesteps = timesteps
        self.gdf_util = gdf_util
        self.ema = ema

    def train_step(self, images):
        # 1. Get the batch size
        batch_size = tf.shape(images)[0]

        # 2. Sample timesteps uniformly
        t = tf.random.uniform(
            minval=0, maxval=self.timesteps, shape=(batch_size,), dtype=tf.int64
        )

        with tf.GradientTape() as tape:
            # 3. Sample random noise to be added to the images in the batch
            noise = tf.random.normal(shape=tf.shape(images), dtype=images.dtype)

            # 4. Diffuse the images with noise
            images_t = self.gdf_util.q_sample(images, t, noise)

            # 5. Pass the diffused images and time steps to the network
            pred_noise = self.network([images_t, t], training=True)

            # 6. Calculate the loss
            loss = self.loss(noise, pred_noise)

        # 7. Get the gradients
        gradients = tape.gradient(loss, self.network.trainable_weights)

        # 8. Update the weights of the network
        self.optimizer.apply_gradients(zip(gradients, self.network.trainable_weights))

        # 9. Updates the weight values for the network with EMA weights
        for weight, ema_weight in zip(self.network.weights, self.ema_network.weights):
            ema_weight.assign(self.ema * ema_weight + (1 - self.ema) * weight)

        # 10. Return loss values
        return {"loss": loss}

    def generate_images(self, num_images=16):
        # 1. Randomly sample noise (starting point for reverse process)
        samples = tf.random.normal(
            shape=(num_images, img_size, img_size, img_channels), dtype=tf.float32
        )
        # 2. Sample from the model iteratively
        for t in reversed(range(0, self.timesteps)):
            tt = tf.cast(tf.fill(num_images, t), dtype=tf.int64)
            pred_noise = self.ema_network.predict(
                [samples, tt], verbose=0, batch_size=num_images
            )
            samples = self.gdf_util.p_sample(
                pred_noise, samples, tt, clip_denoised=True
            )
        # 3. Return generated samples
        return samples

    def plot_images(
        self, epoch=None, logs=None, num_rows=2, num_cols=8, figsize=(12, 5)
    ):
        """Utility to plot images using the diffusion model during training."""
        generated_samples = self.generate_images(num_images=num_rows * num_cols)
        generated_samples = (
            tf.clip_by_value(generated_samples * 127.5 + 127.5, 0.0, 255.0)
            .numpy()
            .astype(np.uint8)
        )

        _, ax = plt.subplots(num_rows, num_cols, figsize=figsize)
        for i, image in enumerate(generated_samples):
            if num_rows == 1:
                ax[i].imshow(image)
                ax[i].axis("off")
            else:
                ax[i // num_cols, i % num_cols].imshow(image)
                ax[i // num_cols, i % num_cols].axis("off")

        plt.tight_layout()
        plt.show()


# Build the unet model
network = build_model(
    img_size=img_size,
    img_channels=img_channels,
    widths=widths,
    has_attention=has_attention,
    num_res_blocks=num_res_blocks,
    norm_groups=norm_groups,
    activation_fn=keras.activations.swish,
)
ema_network = build_model(
    img_size=img_size,
    img_channels=img_channels,
    widths=widths,
    has_attention=has_attention,
    num_res_blocks=num_res_blocks,
    norm_groups=norm_groups,
    activation_fn=keras.activations.swish,
)
ema_network.set_weights(network.get_weights())  # Initially the weights are the same

# Get an instance of the Gaussian Diffusion utilities
gdf_util = GaussianDiffusion(timesteps=total_timesteps)

# Get the model
model = DiffusionModel(
    network=network,
    ema_network=ema_network,
    gdf_util=gdf_util,
    timesteps=total_timesteps,
)

# Compile the model
model.compile(
    loss=keras.losses.MeanSquaredError(),
    optimizer=keras.optimizers.Adam(learning_rate=learning_rate),
)

# Train the model
model.fit(
    train_ds,
    epochs=num_epochs,
    batch_size=batch_size,
    callbacks=[keras.callbacks.LambdaCallback(on_epoch_end=model.plot_images)],
)

```

#### çµæœ
- åœ¨V100 GPU ä¸Šè¨“ç·´äº†é€™å€‹æ¨¡å‹ 800 å€‹ epochï¼Œ æ¯å€‹ epoch éœ€è¦å°‡è¿‘ 8 ç§’æ‰èƒ½å®Œæˆ

```python
!curl -LO https://github.com/AakashKumarNain/ddpms/releases/download/v3.0.0/checkpoints.zip
!unzip -qq checkpoints.zip

# Load the model weights
model.ema_network.load_weights("checkpoints/diffusion_model_checkpoint")

# Generate and plot some samples
model.plot_images(num_rows=4, num_cols=8)
```

- https://github.com/AakashKumarNain/diffusion_models

```python

```



```python

```
