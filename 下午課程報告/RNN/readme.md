![RNN.png](RNN.png)

## RNN èˆ‡ è‡ªç„¶èªžè¨€è™•ç†|å ±å‘Šä¸»é¡Œ
- éœ€æ¶µè“‹åº•ä¸‹`1`åˆ°`5`å…§å®¹(æœªç·¨è™Ÿè€…(åŠ åˆ†é …):å¯ä¸åš)
- `1`.NLPç ”ç©¶ä¸»é¡Œ
  - https://www.geeksforgeeks.org/natural-language-processing-nlp-tutorial/ 
- Text Preprocessing(æ–‡æœ¬é è™•ç†) | Text Feature Engineering
  - [Text Preprocessing(æ–‡æœ¬é è™•ç†)åŸºæœ¬è§€å¿µ](TextPreprocessing.md)
  - [ä½¿ç”¨NLTK](IMDbæ–‡æœ¬åˆ†é¡ž_NLTK.md)
    - ç§»é™¤Stop Wordsï¼ˆåœç”¨è©žï¼‰
    - Text Normalization ==> `è©žå¹¹æå–(stemming)`å’Œ`è©žå½¢é‚„åŽŸ(lemmatization)`
- `2`.[Text Representation or Text Embedding Techniques(word ==> vector) ](NLP_WordVector.md)
  - https://medium.com/ml-note/word-embedding-3ca60663999d
  - Bag of Word
  - TF-IDF
  - Word Embedding 
- `3`.RNN model  ==> The Problem of Long-Term Dependencies
  - Vanilla RNN
  - LSTM|Long Short-Term Memory|é•·çŸ­æœŸè¨˜æ†¶(1997)
    - ã€å°Žè®€ã€‘[Understanding LSTM Networks(2015)](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
    - https://ithelp.ithome.com.tw/articles/10193924 
  - GRU | Gated Recurrent Unit(2014)
    - https://zhuanlan.zhihu.com/p/20310288990
    - https://ithelp.ithome.com.tw/articles/10194201 
  - https://www.geeksforgeeks.org/rnn-vs-lstm-vs-gru-vs-transformers/
- `4`.å¾žseq2seq modelåˆ°Transformer
  - Neural Machine Translation èˆ‡ seq2seq model
    - 2014 [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)
    - 2014 [Learning Phrase Representations using RNN Encoderâ€“Decoder for Statistical Machine Translation](https://emnlp2014.org/papers/pdf/EMNLP2014179.pdf)
    - https://research.google/blog/a-neural-network-for-machine-translation-at-production-scale/
  - Attention
    - 2014ã€ç¶“å…¸è«–æ–‡ã€‘ [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)
    - 2015 ã€ç¶“å…¸è«–æ–‡ã€‘[Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025)
    - ðŸ‘ã€å°Žè®€ã€‘[Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)
    - ã€å°Žè®€ã€‘ ==> 28. æ³¨æ„åŠ›æ©Ÿåˆ¶ï¼ˆAttention mechanismï¼‰
  - 2017 [Transformer](Transformer.md)
    - ðŸ‘ðŸ‘ðŸ‘ã€ç¶“å…¸è«–æ–‡ã€‘[Attention Is All You Need](https://arxiv.org/abs/1706.03762)
    - ã€è«–æ–‡ã€‘[[1607.06450] Layer Normalization | Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton](https://arxiv.org/abs/1607.06450)
    - ã€å°Žè®€ã€‘[The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
    - ã€æ¨¡åž‹æ‡‰ç”¨ã€‘[Transformer_HuggingFaceç¯„ä¾‹](Transformer_HuggingFaceç¯„ä¾‹.md)
- `5`.[Pre-Trained Language Models](Pre-Trained_Language_Models.md) ==> LLM
  - **2018 BERT Bidirectional Encoder Representations from Transforme**
    - ã€ç¶“å…¸è«–æ–‡ã€‘[[1810.04805] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
  - 2018 GPT
    - GPT-1
    -  [https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf]()
    - GPT-2
    - GPT-3
    - GPT-3.5
    - GPT-4 
  - LLAMA
    - https://zh.wikipedia.org/zh-tw/LLaMA
    - LLAMA-1(2023å¹´2æœˆ)
    - LLAMA-2(2023å¹´7æœˆ)
    - Code Llama(2023å¹´8æœˆ)
    - LLAMA-3(2024å¹´4æœˆ18æ—¥)
    - Llama-4(2025å¹´4æœˆ5æ—¥) 

- `5`.ç¯„ä¾‹æ‡‰ç”¨NLP|æ–‡æœ¬åˆ†é¡ž(TEXT Classofication)
  - IMDbæ–‡æœ¬åˆ†é¡ž
    - [IMDB Dataset of 50K Movie Reviews|Kaggle](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)
      - IMDB æ•¸æ“šé›†å…·æœ‰ 50K æ¢é›»å½±è©•è«–ï¼Œç”¨æ–¼è‡ªç„¶èªžè¨€è™•ç†æˆ–æ–‡æœ¬åˆ†æžã€‚
      - é€™æ˜¯ä¸€å€‹ç”¨æ–¼äºŒå…ƒæƒ…æ„Ÿåˆ†é¡žçš„æ•¸æ“šé›†ï¼ŒåŒ…å«çš„æ•¸æ“šæ¯”ä»¥å‰çš„åŸºæº–æ•¸æ“šé›†å¤šå¾—å¤šã€‚
      - ä¸€çµ„ 25,000 ç¯‡é«˜åº¦æ¥µæ€§çš„é›»å½±è©•è«–ç”¨æ–¼åŸ¹è¨“ï¼Œ25,000 ç¯‡ç”¨æ–¼æ¸¬è©¦ã€‚
      - å› æ­¤ï¼Œä½¿ç”¨åˆ†é¡žæˆ–æ·±åº¦å­¸ç¿’æ¼”ç®—æ³•é æ¸¬æ­£é¢å’Œè² é¢è©•è«–çš„æ•¸é‡ã€‚
      - æœ‰é—œæ•¸æ“šé›†çš„æ›´å¤šè³‡è¨Šï¼Œè«‹è¨ªå•ä»¥ä¸‹é€£çµ http://ai.stanford.edu/~amaas/data/sentiment/
    - [Towards Tensorflow 2.0 ç³»åˆ—](https://ithelp.ithome.com.tw/users/20119971/ironman/2254?page=1)
    - å‚³çµ±åšæ³•: word2vec | tfidf | bow
      - https://www.kaggle.com/code/jagarapusiva/imdb-movie-reviews-word2vec-tfidf-bow
    - MLP
    - SimpleRNN
    - LSTM
    - GRU
    - å„ç¨®è®Šå½¢LSTM+CNN
    - Keraså®˜æ–¹ç¯„ä¾‹[Bidirectional LSTM on IMDB](https://keras.io/examples/nlp/bidirectional_lstm_imdb/)
    - Keraså®˜æ–¹ç¯„ä¾‹[Text classification with Transformer](https://keras.io/examples/nlp/text_classification_with_transformer/)
    - Keraså®˜æ–¹ç¯„ä¾‹[Text Classification using FNet](https://keras.io/examples/nlp/fnet_classification_with_keras_hub/)
    - TFå®˜æ–¹ç¯„ä¾‹[Text classification from scratch](https://keras.io/examples/nlp/text_classification_from_scratch/)
    - TFå®˜æ–¹ç¯„ä¾‹[Text classification with an RNN ](https://www.tensorflow.org/text/tutorials/text_classification_rnn)
    - TFå®˜æ–¹ç¯„ä¾‹[Classify text with BERT](https://www.tensorflow.org/text/tutorials/classify_text_with_bert)
  - IMDbæ–‡æœ¬åˆ†é¡ž
    - [IMDbæ–‡æœ¬åˆ†é¡ž_NLTK](IMDbæ–‡æœ¬åˆ†é¡ž_NLTK.md)
    - [IMDbæ–‡æœ¬åˆ†é¡ž_æ–‡å­—å‘é‡åŒ–](IMDbæ–‡æœ¬åˆ†é¡ž_æ–‡å­—å‘é‡åŒ–.md)
    - [IMDbæ–‡æœ¬åˆ†é¡ž_å‚³çµ±ä½œæ³•](IMDbæ–‡æœ¬åˆ†é¡ž_å‚³çµ±ä½œæ³•.md)
      - æ¨¡åž‹1 ==> ä½¿ç”¨CountVectorizer + MultinomialNB
      - æ¨¡åž‹2 ==> ä½¿ç”¨TfidfVectorizer + MultinomialNB
      - æ¨¡åž‹3 ==> ä½¿ç”¨ wordvector + SVC | RandomForestClassifier
    - [IMDbæ–‡æœ¬åˆ†é¡ž_RNN](IMDbæ–‡æœ¬åˆ†é¡ž_RNN.md)
    - Sentiment Analysis æƒ…ç·’åˆ†æž ==> è² è©• vs æ­£è©•
  - ä»¥å…§å®¹ç‚ºåŸºç¤Žçš„æŽ¨è–¦ç³»çµ±
  - [ä¸»é¡Œå»ºæ¨¡Topic Modeling](TopicModelingä¸»é¡Œå»ºæ¨¡.md)

### å…¶ä»–åˆ†æž
- [NLP Text Preprocessing Tutorial](https://www.kaggle.com/code/rudraneelsannigrahi/nlp-text-preprocessing-tutorial)

## æ•™ç§‘æ›¸ç›¸é—œç« ç¯€
#### æ•™ç§‘æ›¸:ç¬¬15ç« Processing Sequences Using RNNs and CNNs ==> [æ™‚é–“åºåˆ—åˆ†æž](æ™‚é–“åºåˆ—åˆ†æž.md)
- Recurrent Neurons and Layers
- Training RNNs
- Forecasting a Time Series
- Handling Long Sequences

#### æ•™ç§‘æ›¸:ç¬¬16ç« 
- Generating Shakespearean Text Using a Character RNN
- Sentiment Analysis
- An Encoderâ€“Decoder Network for Neural Machine Translation
- Attention Mechanisms
- An Avalanche of Transformer Models
- Vision Transformers
- Hugging Faceâ€™s Transformers Library

## å»¶ä¼¸é–±è®€
- [Natural Language Processing in Action, Second Edition(2025)](https://learning.oreilly.com/library/view/natural-language-processing/9781617299445/)
- [Natural Language Processing with Transformers, Revised Edition(2022)](https://learning.oreilly.com/library/view/natural-language-processing/9781098136789/)
- [Python Natural Language Processing Cookbook - Second Edition(2024)](https://learning.oreilly.com/library/view/python-natural-language/9781803245744/)
- [Practical Natural Language Processing(2020)](https://learning.oreilly.com/library/view/practical-natural-language/9781492054047/)
- å‚³çµ±NLP ==> NLTK, spaCy, sklearn, and gensim
  - [Python Natural Language Processing Cookbook(2021)](https://learning.oreilly.com/library/view/python-natural-language/9781838987312/)
  - [Natural Language Processing: Python and NLTK(2016)](https://learning.oreilly.com/library/view/natural-language-processing/9781787285101/) 


