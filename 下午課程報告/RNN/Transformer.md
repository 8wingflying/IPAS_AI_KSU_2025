### Transformer
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- Transformerçš„æ¶æ§‹ï¼Œä¸»è¦ç”±å…©å€‹éƒ¨åˆ†çµ„æˆï¼šç·¨ç¢¼å™¨ï¼ˆEncoderï¼‰ å’Œ è§£ç¢¼å™¨ï¼ˆDecoderï¼‰
- ç·¨ç¢¼å™¨ï¼ˆEncoder): å…ƒä»¶èˆ‡ç”¨é€”
  - Input Embeddingï¼šè¼¸å…¥çš„åºåˆ—ï¼ˆä¾‹å¦‚è©å½™ï¼‰æœƒå…ˆè½‰æ›æˆåµŒå…¥å‘é‡ï¼ˆembeddingï¼‰==> è©çš„å‘é‡è¡¨ç¤ºã€‚
  - Positional Encoding(ä½ç½®ç·¨ç¢¼)ï¼š
    - ç”±æ–¼ Transformer æœ¬èº«ä¸å…·å‚™åºåˆ—çš„æ™‚é–“æ­¥æ¦‚å¿µï¼Œå› æ­¤éœ€è¦å¼•å…¥ä½ç½®ç·¨ç¢¼ï¼ˆPositional Encodingï¼‰ä¾†è®“æ¨¡å‹äº†è§£è¼¸å…¥åºåˆ—çš„é †åºã€‚
    - é€šéçµåˆä¸€ç¨®ç°¡å–®çš„ä½ç½®ç·¨ç¢¼æ–¹æ³•ï¼Œå¼·èª¿äº†ä½ç½®è³‡è¨Šåœ¨ Transformer æ¨¡å‹ä¸­çš„é—œéµä½œç”¨ï¼Œç¢ºä¿æˆ‘å€‘çš„æ¨¡å‹èƒ½å¤ è­˜åˆ¥åºåˆ—ä¸­å…ƒç´ çš„é †åºã€‚ 
  - Multi-Head Attentionï¼šé€™æ˜¯ Transformer çš„æ ¸å¿ƒæ©Ÿåˆ¶ï¼Œåˆ©ç”¨**å¤šé ­è‡ªæ³¨æ„åŠ›ï¼ˆMulti-Head Self-Attentionï¼‰**ä¾†è®“æ¯å€‹è¼¸å…¥è©å½™èˆ‡å…¶ä»–æ‰€æœ‰è©é€²è¡Œé—œè¯ï¼Œæ•æ‰æ•´å€‹åºåˆ—çš„ä¾è³´é—œä¿‚ã€‚
  - Add & Norm(Skip Connections å’Œ Layer Normalization)ï¼š
    - ä½¿ç”¨`æ®˜å·®é€£æ¥`ï¼Œå°‡å¤šé ­æ³¨æ„åŠ›çš„è¼¸å‡ºå’Œè¼¸å…¥ç›¸åŠ ï¼Œä¸¦é€²è¡Œ`Layer æ­£è¦åŒ–`ã€‚
    - æ•´åˆé€™äº›åŸºæœ¬å…ƒä»¶ä»¥ç¢ºä¿æ¨¡å‹çš„è¨“ç·´ç©©å®šæ€§å’Œæ•ˆç‡ï¼Œå±•ç¤ºäº†å®ƒå€‘åœ¨ä¿ƒé€²æ·±åº¦æ¶æ§‹ä¸­æœ‰æ•ˆå­¸ç¿’çš„ä½œç”¨ã€‚
  - Feed Forwardï¼šæ¯å€‹ç·¨ç¢¼å™¨å±¤éƒ½æœ‰ä¸€å€‹å…¨é€£æ¥å±¤ä¾†é€²ä¸€æ­¥è™•ç†æ•¸æ“šã€‚ 
- è§£ç¢¼å™¨ï¼ˆDecoderï¼‰
  - è§£ç¢¼å™¨çµæ§‹èˆ‡ç·¨ç¢¼å™¨é¡ä¼¼
  - æ–°å¢ Masked Multi-Head Attentionã€‚é€™æ˜¯ç‚ºäº†ç¢ºä¿åœ¨è¨“ç·´ç”Ÿæˆæ™‚ï¼Œè§£ç¢¼å™¨åªèƒ½çœ‹åˆ°ä¹‹å‰çš„è¼¸å‡ºï¼Œè€Œä¸èƒ½çœ‹åˆ°æœªä¾†çš„è¼¸å‡ºï¼Œé¿å…æ•¸æ“šæ´©æ¼ã€‚
  - è§£ç¢¼å™¨çš„è¼¸å‡º==> é€šéä¸€å€‹ç·šæ€§å±¤ï¼Œå†é€²è¡Œ Softmaxï¼Œç”Ÿæˆå°æ‡‰çš„æ©Ÿç‡åˆ†å¸ƒï¼Œä»£è¡¨æ¨¡å‹å°ä¸‹ä¸€å€‹è©çš„é æ¸¬ã€‚ 
- åŸºæœ¬æ§‹å»ºå–®å…ƒ ==> ç¸®æ”¾é»ç©æ³¨æ„åŠ›ï¼ˆscaled dot-product attentionï¼‰å–®å…ƒ

### `1`.ã€æ¶æ§‹ã€‘å°è®€
- ğŸ‘[The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
  -  ã€æ•™å­¸å½±ç‰‡(é™é‡å…è²»ç‰ˆ20250610)ã€‘[How Transformer LLMs Work](https://www.deeplearning.ai/short-courses/how-transformer-llms-work/?utm_campaign=handsonllm-launch&utm_medium=partner) 
- ğŸ‘[Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)
- https://ithelp.ithome.com.tw/articles/10363257

### `2`.ã€æ¶æ§‹ã€‘å¯¦ä½œ==> æ ¸å¿ƒé—œéµæŠ€è¡“çš„ç¨‹å¼å¯¦ä½œ[Optional]
### `3`.Transformerå·¥ç¨‹å­¸[Optional]
- Transfer Learning
  - ã€ç¯„ä¾‹ã€‘ [Transfer learning with Transformers trainer and pipeline for NLP](https://billtcheng2013.medium.com/transfer-learning-with-transformers-trainer-and-pipeline-for-nlp-8b1d2c1a8c3d)
  - ã€REVIEWã€‘202410[Transfer Learning on Transformers for Building Energy Consumption Forecasting -- A Comparative Study](https://arxiv.org/abs/2410.14107) 
- Fine-Tuning
#  `4`.ç¯„ä¾‹å­¸ç¿’==>æ‡‰ç”¨
#### ç¯„ä¾‹å­¸ç¿’:[Transformer_HuggingFaceç¯„ä¾‹](Transformer_HuggingFaceç¯„ä¾‹.md)
#### ä½œæ¥­ ==> å…¥é–€æ¨è–¦ å®Œæˆ [å¿«é€Ÿå…¥é–€Quick tour](https://github.com/huggingface/notebooks/blob/main/transformers_doc/en/quicktour.ipynb)
#### ç¯„ä¾‹å­¸ç¿’: å¯¦ä½œä¸€å€‹Keras Transformer
- Attention Layers in TensorFlow
  - Self-Attention (Scaled Dot-Product Attention) --> tf.keras.layers.Attention
  - Multi-Head Attention  --> tf.keras.layers.MultiHeadAttention
  - https://www.geeksforgeeks.org/attention-layers-in-tensorflow/
- [Text classification with Transformer](https://keras.io/examples/nlp/text_classification_with_transformer/)

#### ç¯„ä¾‹: Tensorflow  Transformer
- [Neural machine translation with attention](https://www.tensorflow.org/text/tutorials/nmt_with_attention)
- [Neural machine translation with a Transformer and Keras](https://www.tensorflow.org/text/tutorials/transformer)

#### ç¯„ä¾‹: å¯¦ä½œä¸€å€‹PyTorch Transformer
- [Building a Simple Transformer using PyTorch [Code Included]](https://pureai.substack.com/p/building-a-simple-transformer-using-pytorch)
- https://github.com/ermattson/pure-ai-tutorials/tree/main/SimpleTransformer-PyTorch
- [Natural Language Processing with Transformers, Revised Edition](https://learning.oreilly.com/library/view/natural-language-processing/9781098136789/)
   - https://github.com/nlp-with-transformers/notebooks
   - 01_introduction.ipynb

#### æ›´å¤šç¯„ä¾‹
- https://fancyerii.github.io/2020/07/08/huggingface-transformers/

# `5`.å»¶ä¼¸é–±è®€
## ã€REVIEWã€‘
- [A Survey of Transformers](https://arxiv.org/abs/2106.04554)
- [A Survey on Visual Transformer](https://arxiv.org/abs/2012.12556)
- [Transformers in Time Series: A Survey](https://arxiv.org/abs/2202.07125)
- [Long Range Arena: A Benchmark for Efficient Transformers](https://arxiv.org/abs/2011.04006)

## åƒè€ƒæ›¸
- [Natural Language Processing with Transformers, Revised Edition](https://learning.oreilly.com/library/view/natural-language-processing/9781098136789/)
  - https://github.com/nlp-with-transformers/notebooks
  - ç¬¬ä¸€ç‰ˆç°¡é«”ä¸­è­¯æœ¬
- [Hands-On Generative AI with Transformers and Diffusion Models](https://learning.oreilly.com/library/view/hands-on-generative-ai/9781098149239/)
  - https://github.com/genaibook/genaibook
- [Mastering Transformers - Second Edition](https://learning.oreilly.com/library/view/mastering-transformers/9781837633784/)
  - https://github.com/PacktPublishing/Mastering-Transformers-Second-Edition
- [Transformers for Natural Language Processing and Computer Vision - Third Edition](https://learning.oreilly.com/library/view/transformers-for-natural/9781805128724/)
  - https://github.com/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/
- [Hands-On Large Language Models](https://learning.oreilly.com/library/view/hands-on-large-language/9781098150952/)
  - [ç« ç¯€å…§å®¹](LLM_BOOK_Content.md) 
