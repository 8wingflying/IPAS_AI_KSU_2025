### Transformer
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- 基本構建單元 ==> 縮放點積注意力（scaled dot-product attention）單元

### 導讀
