## 第 2章 處理文本資料
- 2.1 詞嵌入 (Word Embedding，或稱詞向量)
- 2.2 文本斷詞 (Tokenize)
- 2.3 將 token 轉換為 token ID
- 2.4 添加特殊 token
- 2.5 字節對編碼 (Byte Pair Encoding, BPE)
- 2.6 使用滑動視窗法 (Sliding Window) 進行資料取樣
- 2.7 創建 token 嵌入 (Embedding)
- 2.8 位置資訊的編碼 

#### 2.5位元組對編碼(Byte Pair Encoding, BPE)
- GPT-2 使用 BytePair 編碼 (BPE) 作為其標記器
- 它允許模型將不在其預定義詞彙表中的單字分解為更小的子詞單元甚至單個字符，從而使其能夠處理詞彙表之外的單詞
- 例如，如果 GPT-2 的詞彙表中沒有“unfamiliarword”這個詞，它可能會將其標記為 [“unfam”、“iliar”、“word”] 或其他子詞分解，這取決於其訓練過的 BPE 合併
- 原始的 BPE 標記器可以在這裡找到：https://github.com/openai/gpt-2/blob/master/src/encoder.py
- 使用 OpenAI 開源tiktoken庫中的 BPE 分詞器，該庫使用 Rust 實現其核心演算法，以提高計算效能

```python
!pip install tiktoken

import importlib
import tiktoken

print("tiktoken version:", importlib.metadata.version("tiktoken"))

tokenizer = tiktoken.get_encoding("gpt2")

text = (
    "Hello, do you like tea? <|endoftext|> In the sunlit terraces"
     "of someunknownPlace."
)

integers = tokenizer.encode(text, allowed_special={"<|endoftext|>"})

print(integers)

strings = tokenizer.decode(integers)

print(strings)
```
- Fig 2.11 BPE 分詞器將未知詞分解為子詞和單字。
- 這樣，BPE 分詞器就可以解析任何單詞，而無需用特殊標記（例如）替換未知詞<|unk|>

#### 2.6使用滑動視窗的資料採樣
- 為 LLM 建立嵌入的下一步是產生訓練 LLM 所需的輸入-目標對

```python
with open("the-verdict.txt", "r", encoding="utf-8") as f:
    raw_text = f.read()

enc_text = tokenizer.encode(raw_text)
print(len(enc_text))

```
- Fig 2.12給定一個文字樣本，提取輸入區塊作為子樣本，作為 LLM 的輸入。
- LLM 在訓練期間的預測任務是預測輸入區塊之後的下一個單字。
- 在訓練期間，我們會屏蔽掉所有超出目標範圍的單字。
- 需要注意的是，圖中所示的文字必須經過分詞後才能被 LLM 處理；但為了清晰起見，本圖省略了分詞步驟。

```python
enc_sample = enc_text[50:]

context_size = 4

x = enc_sample[:context_size]
y = enc_sample[1:context_size+1]

print(f"x: {x}")
print(f"y:      {y}")

```

- 圖 2.13為了實現高效率的資料載入器，我們將輸入收集到一個張量中x，其中每一行代表一個輸入上下文。
- 第二個張量y包含對應的預測目標（下一個字），它們是透過將輸入移動一個位置而創建的。

#### 2.7 建立`token embeddings(標記嵌入)`
- 圖 2.15準備工作
- 包括對文字進行分詞、將文字分詞轉換為分詞 ID，以及將分詞 ID 轉換為嵌入向量。
- 利用先前創建的分詞 ID 來創建分詞嵌入向量。

- 圖 2.16嵌入層執行查找操作，從嵌入層的權重矩陣中檢索與標記 ID 對應的嵌入向量。
- 例如，標記 ID 5 的嵌入向量位於嵌入層權重矩陣的第六行（之所以是第六行而不是第五行，是因為 Python 從 0 開始計數）。
- 我們假設標記 ID 是由 2.3 節中的小詞彙表產生的。

#### 2.8 編碼字位置
- 圖 2.17嵌入層將 token ID 轉換為相同的向量表示，無論它位於輸入序列的哪個位置。
- 例如，token ID 5，無論它位於 token ID 輸入向量的第一個位置還是第四個位置，都會產生相同的嵌入向量。

- 圖 2.18位置嵌入被加入到標記嵌入向量中，以建立 LLM 的輸入嵌入。
- 位置向量的維度與原始標記嵌入相同。為簡單起見，標記嵌入的值顯示為 1。

- 圖 2.19作為輸入處理流程的一部分，輸入文字首先被分解成單一標記。
- 然後，使用詞彙表將這些標記轉換為標記 ID。
- 標記 ID 被轉換為嵌入向量，並向其添加大小相似的位置嵌入，從而產生用於主要 LLM 層的輸入嵌入。

#### 總結
- 由於 LLM 無法處理原始文本，因此需要將文本資料轉換為數值向量（稱為嵌入）。
- 嵌入將離散資料（例如單字或圖像）轉換為連續向量空間，使其與神經網路操作相容。
- 第一步，將原始文字分解成標記（token），可以是單字或字元。然後，將標記轉換為整數表示，稱為標記 ID。
- 可以添加特殊標記（例如<|unk|>和<|endoftext|>）來增強模型的理解並處理各種上下文，例如未知單字或標記不相關文字之間的邊界。
- 用於 GPT-2 和 GPT-3 等 LLM 的位元組對編碼 (BPE) 標記器可以透過將未知單字分解為子單字單元或單字元來有效地處理它們。
- 對標記化資料使用滑動視窗方法來產生 LLM 訓練的輸入-目標對。
- PyTorch 中的嵌入層充當查找操作，檢索與 token ID 對應的向量。產生的嵌入向量提供了 token 的連續表示，這對於訓練 LLM 等深度學習模型至關重要。
- 雖然 token 嵌入為每個 token 提供了一致的向量表示，但它們缺乏對 token 在序列中位置的感知。
- 為了解決這個問題，有兩種主要的位置嵌入類型：絕對嵌入和相對嵌入。
- OpenAI 的 GPT 模型利用絕對位置嵌入，這些嵌入被添加到 token 嵌入向量中，並在模型訓練期間進行最佳化。
