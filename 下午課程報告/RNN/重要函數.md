# 重要函數
- [sklearn.feature_extraction](https://scikit-learn.org/stable/api/sklearn.feature_extraction.html)
  - 圖片`特徵`處理函數
    - image.PatchExtractor
    - image.extract_patches_2d
    - image.grid_to_graph
    - image.img_to_graph
    - image.reconstruct_from_patches_2d 
  - 文本`特徵`處理函數
    - text.CountVectorizer
    - text.HashingVectorizer
    - text.TfidfTransformer
    - text.TfidfVectorizer 

### text.CountVectorizer
```
The CountVectorizer is a feature extraction tool in natural language processing (NLP) provided by libraries like scikit-learn in Python. 
It converts a collection of text documents into a matrix of token counts, which is often used as input for machine learning models.

Here’s a quick overview of how it works:
Tokenization: Splits the text into individual words (tokens).
Vocabulary Building: Creates a vocabulary of unique words from the corpus.
Count Representation: Represents each document as a vector of word counts.

Example Usage in Python

複製程式碼
from sklearn.feature_extraction.text import CountVectorizer

# Sample text data
documents = [
    "I love programming in Python",
    "Python programming is fun",
    "I love solving problems with Python"
]

# Initialize CountVectorizer
vectorizer = CountVectorizer()

# Fit and transform the text data
X = vectorizer.fit_transform(documents)

# Vocabulary
print("Vocabulary:", vectorizer.vocabulary_)

# Dense matrix representation
print("Document-Term Matrix:\n", X.toarray())

Output
Vocabulary: A dictionary mapping each word to its index.

{'i': 0, 'love': 1, 'programming': 2, 'in': 3, 'python': 4, 'is': 5, 'fun': 6, 'solving': 7, 'problems': 8, 'with': 9}
Document-Term Matrix: A matrix where each row corresponds to a document, and each column corresponds to a word's count.


複製程式碼
[[1 1 1 1 1 0 0 0 0 0]
 [0 0 1 0 1 1 1 0 0 0]
 [1 1 0 0 1 0 0 1 1 1]]

Key Parameters
stop_words: Removes common words like "the", "and", etc.
max_features: Limits the number of features (vocabulary size).
ngram_range: Captures sequences of words (e.g., bigrams, trigrams).
This tool is great for preprocessing text data for machine learning tasks like text classification or clustering.
```
# TfidfVectorizer
- https://zh.wikipedia.org/zh-tw/Tf-idf
- 詞頻（tf）是一詞語出現的次數除以該檔案的總詞語數。
- 假如一篇檔案的總詞語數是100個，而詞語「母牛」出現了3次，那麼「母牛」一詞在該檔案中的詞頻就是3/100=0.03。
- 而計算檔案頻率（IDF）的方法是以檔案集的檔案總數，除以出現「母牛」一詞的檔案數。
- 所以，如果「母牛」一詞在1,000份檔案出現過，而檔案總數是10,000,000份的話，其逆向檔案頻率就是lg（10,000,000 / 1,000）=4。
- 最後的tf-idf的分數為0.03 * 4=0.12。
```
The TfidfVectorizer is a feature extraction tool in natural language processing (NLP) provided by libraries like scikit-learn in Python.
It converts a collection of text documents into a matrix of TF-IDF (Term Frequency-Inverse Document Frequency) features, which represent the importance of words in a document relative to a collection of documents.

Here’s a quick overview and example:

Key Features of TfidfVectorizer
TF-IDF Calculation:
Term Frequency (TF): Measures how frequently a term occurs in a document.
Inverse Document Frequency (IDF): Reduces the weight of terms that appear in many documents, emphasizing unique terms.
Formula: $$\text{TF-IDF}(t, d) = \text{TF}(t, d) \times \log\left(\frac{N}{1 + \text{DF}(t)}\right)$$ where:
(t): term
(d): document
(N): total number of documents
(\text{DF}(t)): number of documents containing the term (t)
Preprocessing:

Converts text to lowercase.
Removes stop words (optional).
Tokenizes and applies stemming/lemmatization (if specified).
Sparse Matrix Output:

Produces a sparse matrix for efficient storage and computation.
Example Usage in Python

from sklearn.feature_extraction.text import TfidfVectorizer

# Sample text data
documents = [
    "The quick brown fox jumps over the lazy dog.",
    "Never jump over the lazy dog quickly.",
    "The fox is quick and the dog is lazy."
]

# Initialize TfidfVectorizer
vectorizer = TfidfVectorizer()

# Fit and transform the documents
tfidf_matrix = vectorizer.fit_transform(documents)

# Get feature names (vocabulary)
feature_names = vectorizer.get_feature_names_out()

# Convert the matrix to a dense array for readability
dense_matrix = tfidf_matrix.toarray()

# Display results
print("Feature Names:", feature_names)
print("TF-IDF Matrix:\n", dense_matrix)

Output Explanation
Feature Names: Displays the unique terms extracted from the documents.
TF-IDF Matrix: A numerical representation of the importance of each term in each document.
This tool is widely used in text classification, clustering, and information retrieval tasks.
Let me know if you'd like further clarification or examples!
```
