## 非監督式學習之維度縮減演算法
- 👍[Hands-on Unsupervised Learning Using Python](https://learning.oreilly.com/library/view/hands-on-unsupervised-learning/9781492035633/) 

#### 維度縮減(降維):動機
- 維度的詛咒(the curse of dimensionality)|休斯現象（Hughes phenomenon）:
  - 機器學習和模式識別中，隨著特徵數量增加，模型的性能會先提升，但當特徵數量超過某一臨界值時，性能反而開始下降的現象
  - 由於特徵空間太過龐大，演算法無法有效且高效地對資料進行訓練
- 降維演算法將高維數據投影到低維空間，在刪除冗餘信息的同時保留盡可能多的突出資訊。
- 一旦數據進入低維空間，機器學習演算法就能夠更有效地識別有趣的模式，因為大量的雜訊已經減少。
- 用途1:降維本身就是目標，例如，構建異常檢測系統。
- 用途2:降維本身並不是目的，而是達到另一個目的的手段。例如，降維通常是機器學習管道的一部分，可幫助解決涉及圖像、視頻、語音和文本的大規模、計算成本高昂的問題。

## 類型 
- 類型1.線性投影(Linear Projection)  [sklearn.decomposition](https://scikit-learn.org/stable/api/sklearn.decomposition.html)
  - PCA(principal component analysis)
    - standard PCA ==> from sklearn.decomposition import `PCA`
    - incremental PCA ==>  from sklearn.decomposition import  `IncrementalPCA`
    - sparse PCA  ==>  from sklearn.decomposition import `SparsePCA`
    - MiniBatchSparsePCA ==> from sklearn.decomposition import `MiniBatchSparsePCA`
    - kernel PCA (nonlinear PCA) ==>  from sklearn.decomposition import `KernelPCA`
  - SVD(singular value decomposition) ==>  from sklearn.decomposition import TruncatedSVD
  - Random projection  
    - Gaussian random projection(標準的) ==> from sklearn.random_projection import `GaussianRandomProjection`
    - sparse random projection.  ==> from sklearn.random_projection import `SparseRandomProjection`
- 類型2.流形學習(Manifold Learning)== 非線性維度縮減(nonlinear dimensionality reduction)
  - [sklearn.manifold](https://scikit-learn.org/stable/api/sklearn.manifold.html)
  - 動機: 瑞士捲的啟發
  - isomap ==>from sklearn.manifold import Isomap
  - multidimensional scaling (MDS)
  - locally linear embedding (LLE)
  - t-distributed stochastic neighbor embedding (t-SNE)
  - dictionary learning
  - random trees embedding
  - independent component analysis. 
- 類型3: 其他
  - [sklearn.discriminant_analysis 鑑別性分析](https://scikit-learn.org/stable/api/sklearn.discriminant_analysis.html)
    - LinearDiscriminantAnalysis 線性判別分析 （LDA） 是一種線性分類演算法
      - 在訓練期間，它會學習類之間最具區分性的軸。
      - 然後，這些軸可用於定義將數據投影到的超平面。
      - 這種方法的好處是，投影將使類盡可能相距甚遠，因此 LDA 是一種很好的技術，可以在運行其他分類演算法之前降低維度（除非單獨的 LDA 就足夠了）。
    - QuadraticDiscriminantAnalysis | 二次判別分析演算法

## 導讀
- [劉智皓 (Chih-Hao Liu)](https://tomohiroliu22.medium.com/)
  - https://github.com/tomohiroliu22/Machine-Learning-Algorithm
  - 66個擴散模型Diffusion Models經典論文
  - 深度學習Paper系列(04)：Variational Autoencoder (VAE)
  - 深度學習Paper系列(03)：Generative Adversarial Networks
  - 「新」12個免費修課免費拿證書的自學平台
  - 20個免費修課免費拿證書的自學平台
  - [Mathematics for Machine Learning](https://mml-book.github.io/)

## 進階研讀
- 台大 林軒田教授
  - [機器學習基石上 (Machine Learning Foundations)---Mathematical Foundations ](https://www.coursera.org/learn/ntumlone-mathematicalfoundations)
  - [機器學習基石下 (Machine Learning Foundations)---Algorithmic Foundations](https://www.coursera.org/learn/ntumlone-algorithmicfoundations)
  - [機器學習技法 (Machine Learning Techniques)](https://www.coursera.org/learn/machine-learning-techniques)

## [教科書範例](非監督式學習之維度縮減演算法_教科書範例.md)
- 圖 8-11 ==> MDS、Isomap 和 t-SNE 應用到瑞士卷
## 範例 👍[Hands-on Unsupervised Learning Using Python](https://learning.oreilly.com/library/view/hands-on-unsupervised-learning/9781492035633/) 
