## [scikit-learnæ”¯æ´çš„æ¼”ç®—æ³• 2. Unsupervised learning](https://scikit-learn.org/stable/unsupervised_learning.html)
- 2.1. Gaussian mixture models
- 2.2. Manifold learning|Nonlinear dimensionality reduction | NLDR [WIKI](https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction)
  - NLDR refers to various related techniques that aim to project high-dimensional data onto lower-dimensional latent manifolds, with the goal of either visualizing the data in the low-dimensional space, or learning the mapping (either from the high-dimensional space to the low-dimensional embedding or vice versa) itself.
  - NLDR can be understood as generalizations of linear decomposition methods used for dimensionality reduction, such as singular value decomposition(SVD) and principal component analysis(PCA).
  - ç¯„ä¾‹[Manifold learning on handwritten digits: Locally Linear Embedding, Isomapâ€¦](https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html) 
  - æ•™å­¸å½±ç‰‡[Introduction to Machine Learning - 11 - Manifold learning and t-SNE]()
  - å“ˆä½›å¤§å­¸è«–æ–‡ [The Mathematical Foundations of Manifold Learning()| Luke Melas-Kyriazi](https://arxiv.org/abs/2011.01307)
- ğŸ‘2.3. Clustering
- 2.4. Biclustering: Spectral Co-Clustering | Spectral Biclustering
- ğŸ‘2.5. [Decomposing signals in components (matrix factorization problems)](https://scikit-learn.org/stable/modules/decomposition.html)
  - Principal component analysis (PCA) | Kernel Principal Component Analysis (kPCA)|Truncated singular value decomposition and latent semantic analysis
  - Dictionary Learning | Factor Analysis |Independent component analysis (ICA) | Non-negative matrix factorization (NMF or NNMF) |Latent Dirichlet Allocation (LDA)
- 2.6. Covariance estimation
- ğŸ‘[2.7. Novelty and Outlier Detection](https://scikit-learn.org/stable/modules/outlier_detection.html)
- 2.8. Density Estimation
- ğŸ‘2.9. Neural network models (unsupervised)
