## Unsupervised Learning ==> [scikit-learn支援的非監督學習演算法](scikit-learn支援的非監督學習演算法.md)
- Clustering
- [Association Rule Mining](AssociationRuleLearning.md) ==> [推薦系統](推薦系統.MD)
- Dimensionality Reduction
- 範例學習 ==>https://github.com/tomohiroliu22/Machine-Learning-Algorithm/tree/main
#### Clustering
  - Centroid-based Methods
    - K-Means clustering
    - K-Mode clustering
    - Fuzzy C-Means (FCM) Clustering 
  - Distribution-based Methods
    - Gaussian mixture models(高斯混合模型)
    - Expectation-Maximization Algorithm
    - Dirichlet process mixture models (DPMMs) 
  - Connectivity based methods
    - Hierarchical clustering(階層式分群) 
    - Agglomerative Clustering
    - Divisive clustering
    - Affinity propagation 
  - Density Based methods
    - DBSCAN (Density-Based Spatial Clustering of Applications with Noise)
    - [OPTICS (Ordering Points To Identify the Clustering Structure)](OPTICS.md) 
#### Association Rule Mining
  - Apriori
  - Eclat | Equivalence Class Transformation
  - F-P Growth Algorithm |Frequent Pattern, 
#### Dimensionality Reduction
- 類型1.線性投影(Linear Projection)  [sklearn.decomposition](https://scikit-learn.org/stable/api/sklearn.decomposition.html)
  - PCA(principal component analysis)
    - standard PCA ==> from sklearn.decomposition import `PCA`
    - incremental PCA ==>  from sklearn.decomposition import  `IncrementalPCA`
    - sparse PCA  ==>  from sklearn.decomposition import `SparsePCA`
    - MiniBatchSparsePCA ==> from sklearn.decomposition import `MiniBatchSparsePCA`
    - kernel PCA (nonlinear PCA) ==>  from sklearn.decomposition import `KernelPCA`
  - SVD(singular value decomposition) ==>  from sklearn.decomposition import TruncatedSVD
  - Random projection  [sklearn.random_projection](https://scikit-learn.org/stable/api/sklearn.random_projection.html)
    - Gaussian random projection(標準的) ==> from sklearn.random_projection import `GaussianRandomProjection`
    - sparse random projection.  ==> from sklearn.random_projection import `SparseRandomProjection`
    - johnson_lindenstrauss_min_dim
- 類型2.流形學習(Manifold Learning)== 非線性維度縮減(nonlinear dimensionality reduction)
  - [sklearn.manifold](https://scikit-learn.org/stable/api/sklearn.manifold.html)
  - 動機: 瑞士捲的啟發
  - 流形學習假設==>資料是均勻取樣於一個高維歐氏空間中的低維流形，因此可以從高維取樣資料中找到高維空間中的低維流形，並求出相應的嵌入對映。
  - 流形學習的代表方法有:
  - Isomap ==>from sklearn.manifold import Isomap
    - 機器學習_學習筆記系列(72)：等距特徵映射(Isometric Mapping) 
  - multidimensional scaling (MDS)
  - Locally Linear Embedding (LLE)
    - https://www.geeksforgeeks.org/locally-linear-embedding-in-machine-learning/
    - 機器學習_學習筆記系列(73)：局部線性嵌入演算法(Locally Linear Embedding)
  - t-distributed stochastic neighbor embedding (t-SNE)
    - https://www.mropengate.com/2019/06/t-sne.html
    - https://zhuanlan.zhihu.com/p/148170862
    - 機器學習_學習筆記系列(78)：t-隨機鄰近嵌入法(t-distributed stochastic neighbor embedding)
  - dictionary learning
  - random trees embedding(隨機森林嵌入)
  - Independent Component Analysis (ICA)
    - https://ithelp.ithome.com.tw/articles/10346835
    - 機器學習_學習筆記系列(64)：獨立成分分析(Independent Component Analysis)  
- 類型3: 其他
  - [sklearn.discriminant_analysis 鑑別性分析](https://scikit-learn.org/stable/api/sklearn.discriminant_analysis.html)
    - LinearDiscriminantAnalysis 線性判別分析 （LDA） 
      - 是一種`線性`分類演算法 + 這是 `監督式`學習
      - 在訓練期間，它會學習類別之間最具區分性的軸。
      - 然後，這些軸可用於定義將數據投影到的超平面。
      - 這種方法的好處是，投影將使類盡可能相距甚遠，因此 LDA 是一種很好的技術，可以在運行其他分類演算法之前降低維度（除非單獨的 LDA 就足夠了）。
    - QuadraticDiscriminantAnalysis | 二次判別分析演算法


### 其他:Non-negative Matrix Factorization (NMF)
- https://www.geeksforgeeks.org/non-negative-matrix-factorization/
- https://en.wikipedia.org/wiki/Non-negative_matrix_factorization
- 機器學習_學習筆記系列(67)：非負矩陣分解(Non-negative Matrix Factorization)
```python
import os 
import numpy as np
import logging
import matplotlib.pyplot as plt
from numpy import random
from tqdm.notebook import tqdm
from random import choices

## 載入資料集
from sklearn.datasets import fetch_lfw_people
logging.basicConfig(level=logging.INFO, format='%(asctime)s %(message)s')
lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)
X=lfw_people.data/256
y=lfw_people.target
n_samples, h, w = lfw_people.images.shape
X=X.T
plt.rcParams["figure.figsize"] = (18,18)
plt.gray()
for i in range(48):
    plt.subplot(6, 8, i + 1)
    plt.imshow(X[:,i].reshape((h, w)), cmap=plt.cm.gray)
    plt.xticks(())
    plt.yticks(())
plt.show() 

## 建立模型
K=30
M,N=X.shape
W=np.random.rand(M,K)
H=np.linalg.lstsq(W,X,rcond=None)[0]
H=np.maximum(H, 1e-6)
for t in tqdm(range(1000)):
    top = np.dot(W.T,X)
    bottom = np.dot(np.dot(W.T,W),H)
    H *= top / bottom
    H = np.maximum(H, 1e-6)
    top = np.dot(X,H.T)
    bottom = np.dot(np.dot(W,H),H.T)
    W *= top / bottom
    W = np.maximum(W, 1e-6)

##
X_rec=np.dot(W,H)
plt.rcParams["figure.figsize"] = (18,18)
plt.gray()
for i in range(48):
    plt.subplot(6, 8, i + 1)
    plt.imshow(X_rec[:,i].reshape((h, w)), cmap=plt.cm.gray)
    plt.xticks(())
    plt.yticks(())
plt.show() 
```

