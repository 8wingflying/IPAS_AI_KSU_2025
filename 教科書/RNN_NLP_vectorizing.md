## Agenda
- A.å–®å­—å‘é‡åŒ–ï¼ˆvectorizingï¼‰
  - æ¨™æº–æµç¨‹èˆ‡å¯¦ä½œ == > class Vectorizerçš„æ’°å¯«
  - ä½¿ç”¨tensorflow.keras.layers.TextVectorization
- B.æ–‡å­—è™•ç†æ¨¡å‹
  - ç¯„ä¾‹è³‡æ–™é›†çš„è™•ç†
  - B1.è©è¢‹æ¨¡å‹(bag-of words model)
  - B2.åºåˆ—æ¨¡å‹(sequence model)
#### ç¯„ä¾‹å­¸ç¿’:æ•™ç§‘æ›¸[Keras å¤§ç¥æ­¸ä½](https://www.tenlong.com.tw/products/9789863127017?list_name=srh)
```
æ•™ç§‘æ›¸[Keras å¤§ç¥æ­¸ä½]
æ·±åº¦å­¸ç¿’å…¨é¢é€²åŒ–ï¼ç”¨ Python å¯¦ä½œCNNã€RNNã€GRUã€LSTMã€GANã€VAEã€Transformer
FranÃ§ois Chollet è‘— é»ƒé€¸è¯ã€æ—é‡‡è–‡ è­¯ é»ƒé€¸è¯ å¯©ã€æ–½å¨éŠ˜ç ”ç©¶å®¤ ç›£ä¿®
```
- ç¯„ä¾‹ç¨‹å¼: [åˆ°å®˜æ–¹ç¶²å€ä¸‹è¼‰](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff) [GITHUB](https://github.com/fchollet/deep-learning-with-python-notebooks) 
- PART 4: Tensorflow RNN
  - ç¬¬10ç« ï¼šæ™‚é–“åºåˆ—çš„æ·±åº¦å­¸ç¿’  [ç¯„ä¾‹ç¨‹å¼](https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/chapter10_dl-for-timeseries.ipynb)
  - ğŸ‘ğŸ»ç¬¬11ç« ï¼šæ–‡å­—è³‡æ–™çš„æ·±åº¦å­¸ç¿’
    - 11-1 æ¦‚è¿°è‡ªç„¶èªè¨€è™•ç†(natural language processing, NLP)
    - 11-2 æº–å‚™æ–‡å­—è³‡æ–™
    - 11-3 è¡¨ç¤ºå–®å­—çµ„çš„å…©ç¨®æ–¹æ³•ï¼šé›†åˆ(set)åŠåºåˆ—(sequence)
    - 11-4 [Transformeræ¶æ§‹](https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/chapter11_part03_transformer.ipynb)
    - 11-5 [æ–‡å­—åˆ†é¡ä¹‹å¤–çš„ä»»å‹™-ä»¥Seq2seqæ¨¡å‹ç‚ºä¾‹](https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/chapter11_part04_sequence-to-sequence-learning.ipynb)

## A.å–®å­—å‘é‡åŒ–ï¼ˆvectorizingï¼‰
- å–®å­—å‘é‡åŒ–ï¼ˆvectorizingï¼‰æ˜¯ä¸€ç¨®å°‡æ–‡å­—è½‰æ›æˆæ•¸å€¼å¼µé‡çš„éç¨‹
- æœ‰è¨±å¤šç¨®åšæ³•ï¼Œä½†éƒ½éµå¾ªç›¸åŒçš„æ¨¡æ¿ ==> åƒçœ‹æ•™ç§‘æ›¸åœ–11.1
ï¹’- 1.å°‡æ–‡å­—æ¨™æº–åŒ–standardize:ä½¿å…¶æ›´æ˜“æ–¼è™•ç†ã€‚
    - ä¾‹å¦‚è½‰æ›æˆå°å¯«å­—æ¯ï¼Œæˆ–åˆªé™¤æ¨™é»ç¬¦è™Ÿç­‰ã€‚
  - 2.æ–·è©ï¼ˆtokenizationï¼‰: æŠŠæ–‡å­—åˆ†å‰²æˆä¸€å€‹å€‹å–®å…ƒï¼ˆç¨±ä¹‹ç‚ºtokenï¼‰ï¼Œå¦‚å­—å…ƒã€å–®å­—æˆ–å–®å­—çµ„
ï¹’- 3.æŠŠtoken è½‰æ›æˆä¸€å€‹æ•¸å€¼å‘é‡ï¼ˆå¦‚ï¼š one-hot å‘é‡ï¼‰ã€‚
    - æˆ‘å€‘é€šå¸¸æœƒå…ˆç‚ºè³‡æ–™ä¸­çš„æ‰€æœ‰token åŠ ä¸Šç´¢å¼•
- 11-2-1 æ–‡å­—æ¨™æº–åŒ–ï¼ˆtext standardization)
  - å¾…è™•ç†çš„å…©æ®µå¥å­
```
sunset came. i was staring at the Mexico sky. Isnt nature splendid ï¼Ÿï¼Ÿã€
Sunset came; I started at the Mexico sky. Isnâ€™t natur splendid? J
```
  - æ–‡å­—æ¨™æº–åŒ–ï¼ˆtext standardization)
    - æ–‡å­—æ¨™æº–åŒ–æ˜¯ç‰¹å¾µå·¥ç¨‹çš„ä¸€ç¨®åŸºç¤å½¢å¼ï¼Œç›®çš„æ˜¯æ¶ˆé™¤æ‰ä¸å¸Œæœ›æ¨¡å‹å»è™•ç†çš„ç·¨ç¢¼å·®ç•°ã€‚
    - è©²æ­¥é©Ÿä¸åƒ…å‡ºç¾åœ¨æ©Ÿå™¨å­¸ç¿’é ˜åŸŸï¼Œè‹¥æˆ‘å€‘æƒ³å»ºæ©‹ä¸€å€‹æœå°‹å¼•æ“ï¼Œä¹Ÿè¦åšåŒæ¨£çš„äº‹æƒ…ã€‚
    - æ¨™æº–åŒ–åšæ³•:
      - å°‡æ–‡å­—è½‰æ›æˆå°å¯«å­—æ¯ä¸¦åˆªé™¤æ¨™é»ç¬¦è™Ÿã€‚
      - å°‡ç‰¹æ®Šå­—å…ƒè½‰æ›æˆæ¨™æº–å­—æ¯ï¼Œä¾‹å¦‚ç”¨ã€Œeã€å–ä»£ã€Œå·²ã€ç­‰
      - å­—æ ¹æå–(stemming) ï¼šå°‡ä¸€å€‹è©å½™çš„ä¸åŒè®Šå½¢ï¼ˆå¦‚ä¸€å€‹å‹•è©çš„è©å½¢è®ŠåŒ–ï¼‰è½‰æ›æˆå–®ä¸€çš„é©ç”¨è¡¨ç¤ºæ³•ã€‚
        - was staring å’Œ stared  == >è®Šæˆ stare 
    - å…©å€‹å¥å­æœƒè®Šæˆï¼š
```
sunset came i was staring at the mexico sky isnt nature splendid 
sunset came i stared at the mexico sky isnt nature splendid 
```
- 11-2-2 æ‹†åˆ†æ–‡å­—ï¼ˆæ–·è©tokenization)
  - 3 ç¨®ä¸åŒçš„æ–¹æ³•ä¾†é€²è¡Œtokenization
     - 1.å–®å­—å±¤ç´šçš„tokenization (word-level tokenization) 
       -  token é–“æ˜¯ä»¥ç©ºæ ¼ï¼ˆæˆ–æ¨™é»ç¬¦è™Ÿï¼‰åˆ†éš”çš„å­å­—ä¸²ã€‚
       -  è©²åšæ³•çš„å…¶ä¸­ä¸€ç¨®è®Šå½¢ï¼Œæ˜¯åœ¨é©ç•¶æƒ…æ³ä¸‹é€²ä¸€æ­¥å°‡å–®å­—æ‹†åˆ†æˆã€Œå­å­—ï¼ˆsubwordï¼‰ã€
          -  å°‡staring è¦–ç‚ºã€Œstarï¼‹ ing ã€
          -  å°‡called è¦–ç‚ºã€Œcall+ ed ã€
     - 2.N-gram tokenization: token æ˜¯ç”±N å€‹é€£çºŒå–®å­—æ§‹æˆçš„çµ„åˆ
       - ã€Œthe cat ã€æˆ–ã€Œhe was ã€éƒ½æ˜¯2-gram çš„token(bigram)
     - 3.å­—å…ƒå±¤ç´šçš„tokenization (character-level tokenizationï¼‰ï¼šæ¯å€‹å­—å…ƒå°±æ˜¯ä¸€å€‹token ã€‚
       - å¯¦éš›æ¡ˆä¾‹ä¸­å¾ˆå°‘ä½¿ç”¨
       - åªæœ‰åœ¨ç‰¹å®šæƒ…å¢ƒï¼ˆä¾‹å¦‚ï¹’æ–‡å­—ç”Ÿæˆæˆ–èªéŸ³è¾¨è­˜ï¼‰æ‰æœƒçœ‹åˆ°ã€‚

## æ¨™æº–æµç¨‹èˆ‡å¯¦ä½œ == > class Vectorizerçš„æ’°å¯«
```python

# Natural-language processing: 
# Preparing text data
# Text standardization
# Text splitting (tokenization)
# Vocabulary indexing
# Using the TextVectorization layer

import string

class Vectorizer:
    def standardize(self, text):
        text = text.lower()
        return "".join(char for char in text if char not in string.punctuation)

    def tokenize(self, text):
        text = self.standardize(text)
        return text.split()

    def make_vocabulary(self, dataset):
        self.vocabulary = {"": 0, "[UNK]": 1}
        for text in dataset:
            text = self.standardize(text)
            tokens = self.tokenize(text)
            for token in tokens:
                if token not in self.vocabulary:
                    self.vocabulary[token] = len(self.vocabulary)
        self.inverse_vocabulary = dict(
            (v, k) for k, v in self.vocabulary.items())

    def encode(self, text):
        text = self.standardize(text)
        tokens = self.tokenize(text)
        return [self.vocabulary.get(token, 1) for token in tokens]

    def decode(self, int_sequence):
        return " ".join(
            self.inverse_vocabulary.get(i, "[UNK]") for i in int_sequence)

vectorizer = Vectorizer()

dataset = [
    "I write, erase, rewrite",
    "Erase again, and then",
    "A poppy blooms.",
]

vectorizer.make_vocabulary(dataset)

test_sentence = "I write, rewrite, and still rewrite again"
encoded_sentence = vectorizer.encode(test_sentence)
print(encoded_sentence)

decoded_sentence = vectorizer.decode(encoded_sentence)
print(decoded_sentence)

```
## ä½¿ç”¨tensorflow.keras.layers.TextVectorization
- [tf.keras.layers.TextVectorization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization)
```
tf.keras.layers.TextVectorization(
    max_tokens=None,
    standardize='lower_and_strip_punctuation',
    split='whitespace',
    ngrams=None,
    output_mode='int',
    output_sequence_length=None,
    pad_to_max_tokens=False,
    vocabulary=None,
    idf_weights=None,
    sparse=False,
    ragged=False,
    **kwargs
)
```
```python
from tensorflow.keras.layers import TextVectorization
text_vectorization = TextVectorization(
    output_mode="int",
)
```

```python
import re
import string
import tensorflow as tf

# è‡ªè¨‚çš„æ¨™æº–åŒ–å‡½æ•¸
def custom_standardization_fn(string_tensor):
    lowercase_string = tf.strings.lower(string_tensor) //å°‡å­—ä¸²è½‰æ›æˆå°å¯«
    //ç”¨ç©ºå­—ä¸²å–ä»£æ¨™é»ç¬¦è™Ÿ
    return tf.strings.regex_replace(
        lowercase_string, f"[{re.escape(string.punctuation)}]", "")

# è‡ªè¨‚çš„tokenizationå‡½å¼
def custom_split_fn(string_tensor):
    return tf.strings.split(string_tensor) //ä½¿ç”¨ç©ºæ ¼ä¾†æ‹†åˆ†å­—ä¸²


text_vectorization = TextVectorization(
    output_mode="int",
    standardize=custom_standardization_fn,
    split=custom_split_fn,
)


dataset = [
    "I write, erase, rewrite",
    "Erase again, and then",
    "A poppy blooms.",
]

# ç‚ºä¸€å€‹æ–‡å­—èªæ–™åº«ä¸­çš„å–®å­—å»ºç«‹ç´¢å¼•
# åªéœ€ç”¨ä¸€å€‹èƒ½ç”Ÿæˆå­—ä¸²çš„Datasetç‰©ä»¶ä¾†å‘¼å«TextVectorization å±¤çš„adapt()æ–¹æ³•ï¼š
text_vectorization.adapt(dataset)

# é¡¯ç¤ºè©å½™è¡¨ä¾†
# å¯ä½¿ç”¨text_vectorization.get_vocabulary()å–å¾—é‹ç®—å‡ºçš„è©å½™è¡¨

vocabulary = text_vectorization.get_vocabulary()
test_sentence = "I write, rewrite, and still rewrite again"
encoded_sentence = text_vectorization(test_sentence)
print(encoded_sentence)

# å°ä¸€å€‹å¥å­é€²è¡Œç·¨ç¢¼(è®Šæˆæ•´æ•¸åºåˆ—)ï¼Œç„¶å¾Œå†è§£ç¢¼(å°‡æ•´æ•¸åºåˆ—è½‰æ›å›å–®å­—)
inverse_vocab = dict(enumerate(vocabulary))
decoded_sentence = " ".join(inverse_vocab[int(i)] for i in encoded_sentence)
print(decoded_sentence)
```
# B.æ–‡å­—è™•ç†æ¨¡å‹
- æ ¸å¿ƒé—œéµå•é¡Œ:
  - å¦‚ä½•è¡¨ç¤ºå€‹åˆ¥å–®å­—
  - å¦‚ä½•è¡¨ç¤ºå–®å­—é †åº
- æ–‡å­—è™•ç†æ¨¡å‹
  - è©è¢‹æ¨¡å‹(bag-of words model)
    - æ‹‹æ£„é †åºã€æŠŠæ–‡å­—è¦–ä½œç„¡åºçš„å–®å­—é›†åˆä¾†è™•ç† 
  - åºåˆ—æ¨¡å‹(sequence model):
    - å–®å­—æ‡‰è©²åš´æ ¼æŒ‰ç…§å‡ºç¾çš„é †åºä¾†è™•ç†ï¼Œä¸€æ¬¡è™•ç†ä¸€å€‹
    - RNNå¾ªç’°æ¨¡å‹
    - æ–°çš„æ··åˆçš„æ–¹æ³•ï¼šTransformer æ¶æ§‹
      - Transformer æ˜¯ä¸ç›´æ¥è™•ç†é †åºçš„ï¼Œä½†å®ƒæœƒå°‡å–®å­—ä½ç½®çš„è³‡è¨Šæ³¨å…¥æ‰€è™•ç†çš„è¡¨ç¤ºæ³•ä¸­ã€‚
      - å¦‚æ­¤ä¸€ä¾†ï¼Œå®ƒä¾¿èƒ½çœ‹åˆ°ä¸€å€‹å¥å­ä¸­çš„ä¸åŒéƒ¨åˆ†(ä¸åŒæ–¼RNN)ï¼ŒåŒæ™‚ä»è€ƒæ…®åˆ°é †åº
    - 2015 å¹´æ‰é–‹å§‹æå‡

## ç¯„ä¾‹è³‡æ–™é›†: IMDB movie reviews data(IMDB å½±è©•åˆ†é¡ä»»å‹™)
- ä¸‹è¼‰è³‡æ–™
```
!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz
!tar -xf aclImdb_v1.tar.gz

//åŸæœ¬aclImdbä¸­é‚„æœ‰ä¸€å€‹train/unsup å­ç›®éŒ„ï¼Œä½†ç”¨ä¸åˆ°ä¸ï¼Œæ–¼æ˜¯ä¾¿å°‡å…¶åˆªé™¤
!rm -r aclImdb/train/unsup
!cat aclImdb/train/pos/4077_10.txt
```
- æª¢è¦–aclImdbçš„ç›®éŒ„çµæ§‹
  - train/posä¸­æœ‰12,500 å€‹æ–‡å­—æª”ï¼Œæ¯å€‹æª”æ¡ˆéƒ½åŒ…å«ä¸€ç¯‡å…·æ­£é¢è©•åƒ¹çš„å½±è©•æ–‡å­—ï¼Œå¯ä½œç‚ºè¨“ç·´è³‡æ–™ä½¿ç”¨ã€‚
  - train/negï¼ä¸­æœ‰12,500 å€‹æ–‡å­—æª”ï¼Œæ¯å€‹æª”æ¡ˆéƒ½åŒ…å«ä¸€ç¯‡å…·è² é¢è©•è«–çš„è¨“ç·´è³‡æ–™ï¼ŒåŒæ¨£ä¹Ÿæœ‰12,500 å€‹æª”æ¡ˆã€‚
  - æˆ‘å€‘ä¸€å…±æœ‰25000 å€‹æ–‡å­—æª”èƒ½ç”¨æ–¼è¨“ç·´
  - å¦å¤–ï¼Œç”¨æ–¼æ¸¬è©¦çš„æ–‡å­—æª”åŒæ¨£ä¹Ÿæ˜¯25000 å€‹ 
```python
import os, pathlib, shutil, random

base_dir = pathlib.Path("aclImdb")
val_dir = base_dir / "val"
train_dir = base_dir / "train"

for category in ("neg", "pos"):
    os.makedirs(val_dir / category)
    files = os.listdir(train_dir / category)
    random.Random(1337).shuffle(files)
    //æº–å‚™ä¸€å€‹é©—è­‰é›†ï¼šå–å‡º20% çš„è¨“ç·´æ–‡å­—æª”ï¼Œä¸¦æ”¾åˆ°ä¸€å€‹æ–°çš„ç›®éŒ„ï¼ˆaclImdb/valï¼‰ä¸­
    num_val_samples = int(0.2 * len(files))
    val_files = files[-num_val_samples:]
    for fname in val_files:
        shutil.move(train_dir / category / fname,
                    val_dir / category / fname)
```
- ä½¿ç”¨keras.utils.`text`_dataset_from_directoryæ‰¹æ¬¡è™•ç†æ–‡å­—è³‡æ–™
  - å°±åƒä½¿ç”¨`image`_dataset_from_directory()ä¾†å‰µå»ºå½±åƒåŠå…¶æ¨™ç±¤çš„æ‰¹æ¬¡è³‡æ–™å—
```python
from tensorflow import keras
batch_size = 32

train_ds = keras.utils.text_dataset_from_directory(
    "aclImdb/train", batch_size=batch_size
)
val_ds = keras.utils.text_dataset_from_directory(
    "aclImdb/val", batch_size=batch_size
)
test_ds = keras.utils.text_dataset_from_directory(
    "aclImdb/test", batch_size=batch_size
)

# Displaying the shapes and dtypes of the first batch

for inputs, targets in train_ds:
    print("inputs.shape:", inputs.shape)
    print("inputs.dtype:", inputs.dtype)
    print("targets.shape:", targets.shape)
    print("targets.dtype:", targets.dtype)
    print("inputs[0]:", inputs[0])
    print("targets[0]:", targets[0])
    break
```
# B1.è©è¢‹æ¨¡å‹(bag-of words model)
- è©è¢‹æ³•(The bag-of-words approach): å°‡å–®å­—è¦–ç‚ºä¸€çµ„`é›†åˆ(set)` ==>ç„¡é †åº

## ç¬¬ä¸€ç¨®æ¸¬è©¦:ä½¿ç”¨`Single words (unigrams)` with binary encoding
- unigram çµ„æˆçš„è©è¢‹
  - the cat sat on the mat é€™å€‹å¥å­å°±æœƒè®Šæˆï¼š
  - (â€catâ€,"matâ€,"on"â€™â€sat"â€™â€theâ€)
- ç”¨ä¸€å€‹ç°¡å–®å‘é‡ä¾†å‘ˆç¾æ–‡å­—æ–‡ä»¶ä¸­çš„æ‰€æœ‰å…§å®¹ï¼Œå…¶ä¸­çš„æ¯å€‹é …ç›®å¯ä»¥åæ˜ ç‰¹å®šå–®å­—æ˜¯å¦å­˜åœ¨ã€‚
- ä½¿ç”¨å¤šå…ƒç·¨ç¢¼ï¼ˆmulti-hotç·¨ç¢¼ï¼‰å°±å¯æŠŠæ–‡å­—æ–‡ä»¶ç·¨æˆä¸€å€‹å‘é‡ï¼Œå…¶é•·åº¦ç­‰åŒæ–¼è©å½™è¡¨ä¸­çš„å–®å­—æ•¸é‡ã€‚
- åœ¨è©²å‘é‡ä¸­ï¼Œå¹¾ä¹æ‰€æœ‰é …ç›®éƒ½æ˜¯ç”¨ã€ŒO ã€ä¾†ä»£è¡¨æ–‡ä»¶ä¸­æœªå‡ºç¾çš„å–®å­—ï¼Œå°‘æ•¸çš„ã€Œl ã€å‰‡ä»£è¡¨é‚£äº›æœ‰å‡ºç¾çš„å–®å­—ã€‚
- Preprocessing datasets with a `TextVectorization` layer
  - max_tokens=20000:
    - é™åˆ¶ä½¿ç”¨20000å€‹æœ€å¸¸å‡ºç¾çš„å–®å­—ã€‚
    - è‹¥æœªæŒ‡å®šå‰‡é è¨­æœƒç‚ºè¨“ç·´è³‡æ–™ä¸­çš„æ¯å›ºå–®å­—å»ºç«‹ç´¢å¼•ï¼Œä½†é€™æ¨£å¯èƒ½æœƒå¤šå‡ºå¹¾è¬å€‹åªå‡ºç¾å…©æ¬¡çš„è©å½™ï¼Œè€Œé€™äº›è©å½™é€šå¸¸æ²’æœ‰ä¹˜è¼‰ä»€éº¼æœ‰ç”¨çš„è³‡è¨Šã€‚
    - 20000æ˜¯åšæ–‡å­—åˆ†é¡æ™‚çš„åˆç†è©å½™é‡
  - å°‡è¼¸å‡ºtoken ç·¨ç¢¼æˆmulti_hotå‘é‡  ==> output_mode="multi_hot"
```python
text_vectorization = TextVectorization(
    max_tokens=20000,
    output_mode="multi_hot",
)

# æº–å‚™å€‹åªç”¢ç”ŸåŸå§‹æ–‡å­—è¼¸å…¥çš„è³‡æ–™é›†(æ²’æœ‰æ¨™ç±¤)
text_only_train_ds = train_ds.map(lambda x, y: x)

# å»ºç«‹ç´¢å¼•:ä½¿ç”¨text_only_train_dsä¾†å»ºç«‹ç´¢å¼•
text_vectorization.adapt(text_only_train_ds)


# num_parallel_calls=4: æ˜ç¢ºæŒ‡å®šnum_parallel_callså€¼ä¾†ä½¿ç”¨å¤šå€‹CPU æ ¸å¿ƒ
binary_1gram_train_ds = train_ds.map(
    lambda x, y: (text_vectorization(x), y),
    num_parallel_calls=4)
binary_1gram_val_ds = val_ds.map(
    lambda x, y: (text_vectorization(x), y),
    num_parallel_calls=4)
binary_1gram_test_ds = test_ds.map(
    lambda x, y: (text_vectorization(x), y),
    num_parallel_calls=4)
        
# æª¢è¦–è¼¸å‡ºçµæœ(the output of binary unigram dataset)

for inputs, targets in binary_1gram_train_ds:
    print("inputs.shape:", inputs.shape)
    print("inputs.dtype:", inputs.dtype)
    print("targets.shape:", targets.shape)
    print("targets.dtype:", targets.dtype)
    print("inputs[0]:", inputs[0])
    print("targets[0]:", targets[0])
    break

# å»ºæ§‹å¯é‡è¤‡ä½¿ç”¨çš„å»ºæ¨¡å‡½å¼get_model()

from tensorflow import keras
from tensorflow.keras import layers

def get_model(max_tokens=20000, hidden_dim=16):
    inputs = keras.Input(shape=(max_tokens,))
    x = layers.Dense(hidden_dim, activation="relu")(inputs)
    x = layers.Dropout(0.5)(x)
    outputs = layers.Dense(1, activation="sigmoid")(x)
    model = keras.Model(inputs, outputs)
    model.compile(optimizer="rmsprop",
                  loss="binary_crossentropy",
                  metrics=["accuracy"])
    return model

# Training and testing the binary unigram model

model = get_model()
        
model.summary()
        
callbacks = [
    keras.callbacks.ModelCheckpoint("binary_1gram.keras",
                                    save_best_only=True)
]

model.fit(binary_1gram_train_ds.cache(),
          validation_data=binary_1gram_val_ds.cache(),
          epochs=10,
          callbacks=callbacks)
        
model = keras.models.load_model("binary_1gram.keras")

print(f"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}")
```
- binary_1gram_train_ds.cache():
- å‘¼å«cache()ä»¥åœ¨è¨˜æ†¶é«”ä¸­å°è³‡æ–™é›†é€²è¡Œå¿«å–
- é€™ä½œæ³•åªæœƒåœ¨ç¬¬1 å€‹epcch ä¸­é€²è¡Œé å…ˆè™•ç†ï¼Œä¸¦åœ¨æ¥ä¸‹ä¾†çš„epoch ä¸­é‡ç”¨é å…ˆè™•ç†å®Œçš„æ–‡å­—
- é€™å€‹åšæ³•åªæœ‰åœ¨è³‡æ–™é‡å°åˆ°èƒ½è£é€²è¨˜æ†¶é«”æ™‚æ‰é©ç”¨

## ç¬¬äºŒç¨®æ¸¬è©¦ ==> ä½¿ç”¨`Bigrams` with `binary` encoding
- Bigramsçµ„æˆçš„è©è¢‹
  - the cat sat on the mat é€™å€‹å¥å­å°±æœƒè®Šæˆï¼š
  - {"theâ€,â€the catâ€, â€catâ€,"cat satâ€,"sat","sat on â€,â€onâ€,â€on the","the mat","mat "}
- TextVectorizationå±¤å¯ä»¥å‚³å›ä»»æ„N å€¼çš„N-gramï¼Œåªè¦å°‡å¢rams åƒæ•¸è¨­å®šç‚ºæ‰€éœ€çš„N å³å¯
```python

# Configuring the TextVectorization layer to return bigrams

text_vectorization = TextVectorization(
    ngrams=2,
    max_tokens=20000,
    output_mode="multi_hot",
)

# Training and testing the binary bigram model

text_vectorization.adapt(text_only_train_ds)
binary_2gram_train_ds = train_ds.map(
    lambda x, y: (text_vectorization(x), y),
    num_parallel_calls=4)
binary_2gram_val_ds = val_ds.map(
    lambda x, y: (text_vectorization(x), y),
    num_parallel_calls=4)
binary_2gram_test_ds = test_ds.map(
    lambda x, y: (text_vectorization(x), y),
    num_parallel_calls=4)

model = get_model()
        
model.summary()
        
callbacks = [
    keras.callbacks.ModelCheckpoint("binary_2gram.keras",
                                    save_best_only=True)
]
        
model.fit(binary_2gram_train_ds.cache(),
          validation_data=binary_2gram_val_ds.cache(),
          epochs=10,
          callbacks=callbacks)
        
model = keras.models.load_model("binary_2gram.keras")
        
print(f"Test acc: {model.evaluate(binary_2gram_test_ds)[1]:.3f}")
```
## ç¬¬ä¸‰ç¨®æ¸¬è©¦ ==> Bigrams with `TF-IDF` encoding
- å¯è—‰ç”±è¨ˆç®—æ¯å€‹å–®å­—æˆ–N-gram çš„å‡ºç¾æ¬¡æ•¸ï¼Œç‚ºè¡¨ç¤ºæ³•å†å¢æ·»ä¸€äº›è³‡è¨Šã€‚
- ä½¿ç”¨æ–‡å­—è³‡æ–™ä¸­ï¼Œå–®å­—æˆ–N-gram å‡ºç¾æ¬¡æ•¸çš„ç›´æ–¹åœ“(histogram):
- { "the": 2,â€the cat": l ,â€cat": 1, "cat satâ€: 1, "sat": 1,"sat onâ€: 1 ,â€on": 1, "on the":1,ï¼‚the mat": l, "matâ€: 1}
- é€²è¡Œæ–‡å­—åˆ†é¡æ™‚ï¼ŒçŸ¥é“ç‰¹å®šå–®å­—çš„å‡ºç¾æ¬¡æ•¸éå¸¸é—œéµã€‚
- ä»»ä½•æœ‰ä¸€å®šé•·åº¦çš„å½±è©•éƒ½å¯èƒ½åŒ…å«terrible ä¸€è©ï¼Œä½†å¦‚æœå½±è©•ä¸­å‡ºç¾äº†å¾ˆå¤šæ¬¡terribleï¼Œå°±å¾ˆå¯èƒ½æ˜¯è² é¢å½±è©•ã€‚
- æ³¨æ„åº•ä¸‹æ‰€ä½¿ç”¨çš„output_modeåƒæ•¸
  - output_mode="count" ==> å–®ç´”è¨ˆç®—å‡ºç·šæ¬¡æ•¸
  - output_mode="tf_idf"==> TF-IDF æ­£è¦åŒ–(TF-IDF normalization)
    - å·²ç¶“å…§å»ºåœ¨TextVectorizationå±¤ä¸­:åªè¦å°‡output_mode åƒæ•¸è¨­å®šç‚ºtf_idf,å°±å¯ä½¿ç”¨ 
```python
# Configuring the TextVectorization layer to return token counts

text_vectorization = TextVectorization(
    ngrams=2,
    max_tokens=20000,
    output_mode="count"
)
```
```python
# Configuring TextVectorization to return TF-IDF-weighted outputs

text_vectorization = TextVectorization(
    ngrams=2,
    max_tokens=20000,
    output_mode="tf_idf",
)
```      
#### Training and testing the TF-IDF bigram model
```
text_vectorization.adapt(text_only_train_ds)

tfidf_2gram_train_ds = train_ds.map(
    lambda x, y: (text_vectorization(x), y),
    num_parallel_calls=4)
tfidf_2gram_val_ds = val_ds.map(
    lambda x, y: (text_vectorization(x), y),
    num_parallel_calls=4)
tfidf_2gram_test_ds = test_ds.map(
    lambda x, y: (text_vectorization(x), y),
    num_parallel_calls=4)

model = get_model()
        
model.summary()
        
callbacks = [
    keras.callbacks.ModelCheckpoint("tfidf_2gram.keras",
                                    save_best_only=True)
]
        
model.fit(tfidf_2gram_train_ds.cache(),
          validation_data=tfidf_2gram_val_ds.cache(),
          epochs=10,
          callbacks=callbacks)
        
model = keras.models.load_model("tfidf_2gram.keras")
        
print(f"Test acc: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}")
```
### åŒ¯å‡ºä¸€å€‹è™•ç†åŸå§‹å­—ä¸²çš„æ¨¡å‹
- åœ¨å‰è¿°æ¡ˆä¾‹ä¸­ï¼Œå°‡æ–‡å­—æ¨™æº–åŒ–ã€å–®å­—æ‹†åˆ†å’Œå»ºç«‹ç´¢å¼•éƒ½ç•¶ä½œtf.data å·¥ä½œæµçš„ä¸€éƒ¨åˆ†ã€‚
- ä½†ä¸åŒæ¨¡å‹ä½¿ç”¨å…¶æ–‡å­—è™•ç†å±¤å‰‡...
- ä¹Ÿå°±æ˜¯:è‹¥æˆ‘å€‘æƒ³åŒ¯å‡ºä¸€å€‹ä¸éœ€è¦è©²å·¥ä½œæµçš„ç¨ç«‹é‹ä½œæ¨¡å‹ï¼Œå°±è¦ç¢ºä¿å®ƒæœ‰è‡ªå·±çš„æ–‡å­—è™•ç†å±¤
- ï¼ˆå¦å‰‡å°±è¦åœ¨è³éš›é‹ä½œçš„ç’°å¢ƒä¸­é‡æ–°è™•ç†ä¸€æ¬¡ï¼Œé€™å¾ˆæœ‰æŒ‘æˆ°æ€§ï¼Œä¹Ÿå¯èƒ½å°è‡´è¨“ç·´è³‡æ–™è·Ÿå¯¦éš›é‹ä½œçš„è³‡æ–™ä¹‹é–“çš„å¾®å¦™å·®ç•°ï¼‰ã€‚
- é€™å€‹å•é¡Œä¸é›£è§£æ±ºã€‚åªè¦å‰µå»ºåŒ…å«TextVectorization å±¤çš„ä¸€å€‹æ–°æ¨¡å‹ï¼Œä¸¦åŠ å…¥å‰›å‰›è¨“ç·´çš„æ¨¡å‹å³å¯
```
inputs = keras.Input(shape=(1,), dtype="string")
processed_inputs = text_vectorization(inputs)
outputs = model(processed_inputs)

inference_model = keras.Model(inputs, outputs)
        
import tensorflow as tf

raw_text_data = tf.convert_to_tensor([
    ["That was an excellent movie, I loved it."],
])
        
predictions = inference_model(raw_text_data)
        
print(f"{float(predictions[0] * 100):.2f} percent positive")
```

# B2.åºåˆ—æ¨¡å‹(sequence model)
- Processing words as a sequence: The sequence model approach
- å–®å­—é †åºçš„é‡è¦æ€§ï¼šå°å…·é †åºçš„ç‰¹å¾µé€²è¡Œäººå·¥è™•ç†ï¼ˆå¦‚ï¼š N-gramï¼‰èƒ½è®“æº–ç¢ºåº¦å¤§å¤§æå‡
- æ·±åº¦å­¸ç¿’çš„æ­·å²æ˜¯èˆ‡äººå·¥çš„ç‰¹å¾µå·¥ç¨‹èƒŒé“è€Œé¦³çš„ã€‚
- æ·±åº¦å­¸ç¿’çš„ç›®æ¨™ï¼Œå°±æ˜¯è¦è®“æ¨¡å‹åƒ…å¾çœ‹éçš„è³‡æ–™ä¸­è‡ªå·±å­¸ç¿’ç‰¹å¾µã€‚
- å¦‚æœä¸äººå·¥åšå‡ºåŒ…å«é †åºè³‡è¨Šçš„ç‰¹å¾µï¼Œè¦å¦‚ä½•è®“æ¨¡å‹è™•ç†åŸå§‹å–®å­—åºåˆ—ï¼Œä¸¦è‡ªå·±æ‰¾å‡ºé€™æ¨£çš„ç‰¹å¾µå‘¢ï¼Ÿ
- åºåˆ—æ¨¡å‹sequence model
- å¯¦ä½œä¸€å€‹åºåˆ—æ¨¡å‹
  - 1.å…ˆç”¨æ•´æ•¸ç´¢å¼•åºåˆ—ä¾†è¡¨ç¤ºè¼¸å…¥æ¨£æœ¬ï¼ˆä¸€å€‹æ•´æ•¸ä»£è¡¨ä¸€å€‹å–®å­—ï¼‰
  - 2.å°‡æ¯å€‹æ•´æ•¸å°æ‡‰åˆ°ä¸€å€‹å‘é‡ä»¥å–å¾—å‘é‡åºåˆ—ã€‚
  - 3.æŠŠé€™äº›å‘é‡åºåˆ—é€å…¥èƒ½è®“ã€Œç›¸é„°å‘é‡ä¸­çš„ç‰¹å¾µç”¢ç”Ÿé—œè¯æ€§ã€çš„å †ç–Šå±¤ä¸­ï¼Œä¾‹å¦‚lD å·ç©ç¥ç¶“ç¶²è·¯ã€RNN æˆ–Transformer ç­‰ã€‚
- åœ¨2016 è‡³2017 å¹´å·¦å³çš„ä¸€æ®µæ™‚é–“è£¡ï¼Œé›™å‘RNN ï¼ˆå°¤å…¶æ˜¯é›™å‘LSTMï¼‰è¢«èªç‚ºæ˜¯åºåˆ—æ¨¡å‹çš„æœ€é ‚å°–æˆæœ
- ç¾ä»Šçš„åºåˆ—æ¨¡å‹å¹¾ä¹éƒ½æ˜¯ç”¨Transformer ä¾†å»ºæ§‹
- å¥‡æ€ªçš„æ˜¯,lD å·ç©ç¶²è·¯åœ¨NLPä¸­å¾ä¾†éƒ½ä¸æ˜¯å¾ˆæµè¡Œ

## ç¬¬ä¸€å€‹æ¸¬è©¦:é›™å‘RNN(é›™å‘LSTM)
```python

# Downloading the data

!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz
!tar -xf aclImdb_v1.tar.gz
!rm -r aclImdb/train/unsup
```
# Preparing the data
```python
import os, pathlib, shutil, random
from tensorflow import keras

batch_size = 32

base_dir = pathlib.Path("aclImdb")
val_dir = base_dir / "val"
train_dir = base_dir / "train"

for category in ("neg", "pos"):
    os.makedirs(val_dir / category)
    files = os.listdir(train_dir / category)
    random.Random(1337).shuffle(files)
    num_val_samples = int(0.2 * len(files))
    val_files = files[-num_val_samples:]
    for fname in val_files:
        shutil.move(train_dir / category / fname,
                    val_dir / category / fname)

train_ds = keras.utils.text_dataset_from_directory(
    "aclImdb/train", batch_size=batch_size
)

val_ds = keras.utils.text_dataset_from_directory(
    "aclImdb/val", batch_size=batch_size
)
test_ds = keras.utils.text_dataset_from_directory(
    "aclImdb/test", batch_size=batch_size
)

text_only_train_ds = train_ds.map(lambda x, y: x)
```
# Preparing integer sequence datasets ==>æ•´æ•¸åºåˆ—
- max_length = 600:ç‚ºäº†æ§åˆ¶è¼¸å…¥å¤§å°ï¼Œæˆ‘å€‘åªæœƒæ¡ç”¨è©•è«–ä¸­çš„å‰600å€‹å–®å­—ã€‚
- é€™æ˜¯ä¸€å€‹å¾ˆåˆç†çš„åšæ³•ï¼Œå› ç‚ºå½±è©•çš„å¹³å‡é•·åº¦ç‚º233 å­—,åªæœ‰5% çš„è©•è«–æœƒè¶…é600å­—
```
from tensorflow.keras import layers

max_length = 600
max_tokens = 20000

text_vectorization = layers.TextVectorization(
    max_tokens=max_tokens,
    output_mode="int",
    output_sequence_length=max_length,
)

text_vectorization.adapt(text_only_train_ds)

int_train_ds = train_ds.map(
    lambda x, y: (text_vectorization(x), y),
    num_parallel_calls=4)
int_val_ds = val_ds.map(
    lambda x, y: (text_vectorization(x), y),
    num_parallel_calls=4)
int_test_ds = test_ds.map(
    lambda x, y: (text_vectorization(x), y),
    num_parallel_calls=4)
```
#  ä½¿ç”¨ one-hot encoded vector sequenceså»ºç«‹çš„sequence model
- å»ºæ§‹æ¨¡å‹:è¦æŠŠæ•´æ•¸åºåˆ—è½‰æ›æˆå‘é‡åºåˆ—
  - ä½¿ç”¨æœ€ç°¡å–®çš„æ–¹å¼:å°æ•´æ•¸é€²è¡Œone-hot ç·¨ç¢¼
  - embedded = tf.one_hot(inputs, depth=max_tokens)
  - å†æŠŠé€™äº›one-hot å‘é‡å‚³é€²ä¸€å€‹ç°¡å–®çš„é›™å‘LSTM
- inputs = keras.Input(shape=(None,), dtype="int64")
  - ä¸€ç­†è¼¸å…¥å°±æ˜¯ä¸€å€‹æ•´æ•¸åºåˆ—
  - shape=(None,) è¡¨ç¤ºåºåˆ—é•·åº¦ä¸å›ºå®šï¼Œä¸éæœ¬ä¾‹å¯¦éš›è¼¸å‡ºçš„åºåˆ—é•·åº¦å‡ç‚º600
- embedded = tf.one_hot(inputs, depth=max_tokens) 
  - depth = max_tokens = 20000: å°‡æ¯å€‹æ•´æ•¸å€¼éƒ½ç·¨ç¢¼ç‚º20,000ç¶­çš„one-hot å‘é‡
```
import tensorflow as tf

inputs = keras.Input(shape=(None,), dtype="int64")

embedded = tf.one_hot(inputs, depth=max_tokens)

x = layers.Bidirectional(layers.LSTM(32))(embedded)
x = layers.Dropout(0.5)(x)

outputs = layers.Dense(1, activation="sigmoid")(x)

model = keras.Model(inputs, outputs)

model.compile(optimizer="rmsprop",
              loss="binary_crossentropy",
              metrics=["accuracy"])
              
model.summary()

# Training 

callbacks = [
    keras.callbacks.ModelCheckpoint("one_hot_bidir_lstm.keras",
                                    save_best_only=True)
]

model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)

model = keras.models.load_model("one_hot_bidir_lstm.keras")
print(f"Test acc: {model.evaluate(int_test_ds)[1]:.3f}")
```
æ•™ç§‘æ›¸ä½œè€…çš„è©•è«–:
- 1.é€™å€‹æ¨¡å‹çš„è¨“ç·´é€Ÿåº¦éå¸¸æ…¢
  - é€™æ˜¯å› ç‚ºæˆ‘å€‘çš„è¼¸å…¥ç›¸ç•¶å¤§ï¹’æ¯å€‹è¼¸å…¥æ¨£æœ¬éƒ½è¢«ç·¨ç¢¼æˆä¸€å€‹å¤§å°ç‚ºï¼ˆ600, 20000ï¼‰çš„çŸ©é™£
  - å› æ­¤ä¸€ç¯‡å½±è©•å°±æœ‰12,000,000 å€‹æµ®é»æ•¸ï¼Œå¯è¦‹é›™å‘LSTM çš„å·¥ä½œé‡éå¸¸å¤§
- 2.æ¨¡å‹çš„æ¸¬è©¦æº–ç¢ºåº¦åªé”åˆ°87%ï¼Œé ä¸åŠå‰é¢æ¨¡å‹ã€‚

# ä½¿ç”¨ word embeddingså»ºç«‹çš„sequence model
- word embeddingsçš„å¥½è™•èˆ‡æ­·å²
- ç²å¾—word embeddingsçš„å…©ç¨®æ–¹æ³•:
  - 1.é‡å°æ¬²è§£æ±ºçš„ä»»å‹™ï¼ˆå¦‚æ–‡ä»¶åˆ†é¡æˆ–æƒ…æ„Ÿé æ¸¬ï¼‰é€²è¡Œè©word embeddingsçš„å­¸ç¿’
    - æœƒå¾éš¨æ©Ÿçš„è©å‘é‡é–‹å§‹,ç„¶å¾Œåƒå­¸ç¿’ç¥ç¶“ç¶²è·¯æ¬Šé‡ä¸€æ¨£,å­¸ç¿’å–®å­—å‘é‡çš„è¡¨ç¤ºæ³•
  - 2.é è¨“ç·´è©ä¾å…¥æ³•(pretrained word embedding)
    - å°‡é å…ˆå­¸ç¿’å¥½ï¼ˆä½¿ç”¨èˆ‡ç•¶å‰å•é¡Œä¸åŒçš„æ©Ÿå™¨å­¸ç¿’ä»»å‹™ï¼‰çš„è©åµŒå…¥å‘é‡è¼‰å…¥æ¨¡å‹ä¸­ä½¿ç”¨
- Learning word embeddings with the Embedding layer
```python

# åˆå§‹åŒ–Embedding layer(Instantiating an Embedding layer)

embedding_layer = layers.Embedding(input_dim=max_tokens, output_dim=256)

# é‡æ–°è¨“ç·´Embedding layer(Model that uses an Embedding layer trained from scratch)

inputs = keras.Input(shape=(None,), dtype="int64")

embedded = layers.Embedding(input_dim=max_tokens, output_dim=256)(inputs)

x = layers.Bidirectional(layers.LSTM(32))(embedded)
x = layers.Dropout(0.5)(x)

outputs = layers.Dense(1, activation="sigmoid")(x)

model = keras.Model(inputs, outputs)

model.compile(optimizer="rmsprop",
              loss="binary_crossentropy",
              metrics=["accuracy"])
              
model.summary()

callbacks = [
    keras.callbacks.ModelCheckpoint("embeddings_bidir_gru.keras",
                                    save_best_only=True)
]

model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)

model = keras.models.load_model("embeddings_bidir_gru.keras")
print(f"Test acc: {model.evaluate(int_test_ds)[1]:.3f}")
```

# ä½¿ç”¨masking enabledçš„ word embeddingså»ºç«‹çš„sequence model
- padding and masking ==> Embedding layer with masking enabled
```python

inputs = keras.Input(shape=(None,), dtype="int64")

embedded = layers.Embedding(
    input_dim=max_tokens, output_dim=256, mask_zero=True)(inputs)
    
x = layers.Bidirectional(layers.LSTM(32))(embedded)
x = layers.Dropout(0.5)(x)

outputs = layers.Dense(1, activation="sigmoid")(x)

model = keras.Model(inputs, outputs)

model.compile(optimizer="rmsprop",
              loss="binary_crossentropy",
              metrics=["accuracy"])
              
model.summary()

callbacks = [
    keras.callbacks.ModelCheckpoint("embeddings_bidir_gru_with_masking.keras",
                                    save_best_only=True)
]

model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)

model = keras.models.load_model("embeddings_bidir_gru_with_masking.keras")
print(f"Test acc: {model.evaluate(int_test_ds)[1]:.3f}")
```

## ä½¿ç”¨pretrained word embeddingså»ºç«‹çš„sequence model
- ä½¿ç”¨pretrained word embeddings:GloVe word-embeddings
  - 1.ä¸‹è¼‰pretrained word embeddings:GloVe word-embeddings
  - 2.å»ºæ§‹ GloVe word-embeddings matrix
  - 3.ä½¿ç”¨layers.Embeddingè™•ç†GloVe word-embeddings matrixembedding_matrixä¸¦å»ºç½®æˆembedding_layer
  - 4.ä½¿ç”¨ pretrained Embedding layerå»ºæ§‹æ¨¡å‹

- 1.ä¸‹è¼‰pretrained word embeddings:GloVe word-embeddings
```
!wget http://nlp.stanford.edu/data/glove.6B.zip
!unzip -q glove.6B.zip
```
```
import numpy as np
path_to_glove_file = "glove.6B.100d.txt"

embeddings_index = {}
with open(path_to_glove_file) as f:
    for line in f:
        word, coefs = line.split(maxsplit=1)
        coefs = np.fromstring(coefs, "f", sep=" ")
        embeddings_index[word] = coefs

print(f"Found {len(embeddings_index)} word vectors.")
```
- 2.å»ºæ§‹ GloVe word-embeddings matrix
```
embedding_dim = 100

vocabulary = text_vectorization.get_vocabulary()

word_index = dict(zip(vocabulary, range(len(vocabulary))))

embedding_matrix = np.zeros((max_tokens, embedding_dim))

for word, i in word_index.items():
    if i < max_tokens:
        embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        embedding_matrix[i] = embedding_vector
```        
- 3.ä½¿ç”¨layers.Embedding()è™•ç†GloVe word-embeddings matrixä¸¦å»ºç½®æˆembedding_layer
```        
embedding_layer = layers.Embedding(
    max_tokens,
    embedding_dim,
    embeddings_initializer=keras.initializers.Constant(embedding_matrix),
    trainable=False,
    mask_zero=True,
)
```
- 4.ä½¿ç”¨ pretrained Embedding layerå»ºæ§‹æ¨¡å‹
```
inputs = keras.Input(shape=(None,), dtype="int64")

embedded = embedding_layer(inputs)
x = layers.Bidirectional(layers.LSTM(32))(embedded)
x = layers.Dropout(0.5)(x)

outputs = layers.Dense(1, activation="sigmoid")(x)

model = keras.Model(inputs, outputs)

model.compile(optimizer="rmsprop",
              loss="binary_crossentropy",
              metrics=["accuracy"])
              
model.summary()

callbacks = [
    keras.callbacks.ModelCheckpoint("glove_embeddings_sequence_model.keras",
                                    save_best_only=True)
]

model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)

model = keras.models.load_model("glove_embeddings_sequence_model.keras")
print(f"Test acc: {model.evaluate(int_test_ds)[1]:.3f}")
```
